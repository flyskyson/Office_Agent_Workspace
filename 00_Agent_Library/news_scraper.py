# -*- coding: utf-8 -*-
"""
ä½¿ç”¨ Playwright ç›´æ¥çˆ¬å–çƒ­æ¦œæ•°æ®
ç»•è¿‡ APIï¼Œç›´æ¥è·å–ç½‘é¡µæ•°æ®

æ”¯æŒå¹³å°ï¼š
- å¾®åšçƒ­æœï¼ˆå·²éªŒè¯ï¼‰
- çŸ¥ä¹çƒ­æ¦œ
- ç™¾åº¦çƒ­æœ
"""
import asyncio
import sys
from datetime import datetime
from playwright.async_api import async_playwright
from typing import Dict, List, Optional

# Windows ç¼–ç ä¿®å¤
if sys.platform == 'win32' and hasattr(sys.stdout, 'buffer'):
    import codecs
    sys.stdout = codecs.getwriter('utf-8')(sys.stdout.buffer, 'strict')


class NewsScraper:
    """ä½¿ç”¨ Playwright çˆ¬å–çƒ­æ¦œ"""

    def __init__(self):
        self.supported_platforms = {
            "weibo": {"name": "å¾®åšçƒ­æœ", "func": "scrape_weibo"},
            "zhihu": {"name": "çŸ¥ä¹çƒ­æ¦œ", "func": "scrape_zhihu"},
            "baidu": {"name": "ç™¾åº¦çƒ­æœ", "func": "scrape_baidu"},
        }

    async def scrape_zhihu(self, page, limit=10):
        """çˆ¬å–çŸ¥ä¹çƒ­æ¦œ"""
        try:
            # ä½¿ç”¨ç§»åŠ¨ç«¯é¡µé¢ï¼Œæ— éœ€ç™»å½•
            await page.goto("https://www.zhihu.com/api/v3/feed/topstory/hot-lists/total?limit=10", timeout=15000)

            # ç­‰å¾…å“åº”
            await page.wait_for_load_state("networkidle", timeout=10000)

            # è·å– JSON æ•°æ®
            data = await page.evaluate("() => JSON.parse(document.body.innerText)")

            items = []
            if data and "data" in data:
                for i, item in enumerate(data["data"][:limit], 1):
                    target = item.get("target", {})
                    title = target.get("title", "")
                    url = target.get("url", "")
                    hot = target.get("hot_number", "")

                    # æ ¼å¼åŒ–çƒ­åº¦
                    if hot:
                        hot_num = int(hot)
                        if hot_num > 10000:
                            hot = f"{hot_num / 10000:.1f}ä¸‡"

                    items.append({
                        "title": title.strip(),
                        "url": url,
                        "hot": str(hot) if hot else "",
                        "rank": i
                    })

            return {
                "platform": "çŸ¥ä¹çƒ­æ¦œ",
                "news_list": items,
                "total": len(items),
                "source": "API çˆ¬å–"
            }

        except Exception as e:
            print(f"   âš ï¸ çŸ¥ä¹çˆ¬å–å¤±è´¥: {e}")
            return None

    async def scrape_weibo(self, page, limit=10):
        """çˆ¬å–å¾®åšçƒ­æœ"""
        try:
            await page.goto("https://s.weibo.com/top/summary", timeout=15000)
            await page.wait_for_selector("#pl_top_realtimehot table", timeout=10000)

            items = []
            rows = await page.query_selector_all("#pl_top_realtimehot table tbody tr")

            for i, row in enumerate(rows[:limit], 1):
                # æ ‡é¢˜
                title_el = await row.query_selector("td:nth-child(2) > a")
                if title_el:
                    title = await title_el.inner_text()
                    link = await page.evaluate("(el) => el.href", title_el)

                    # çƒ­åº¦
                    hot_el = await row.query_selector("td:nth-child(2) > span")
                    hot = ""
                    if hot_el:
                        hot = await hot_el.inner_text()

                    items.append({
                        "title": title.strip(),
                        "url": link,
                        "hot": hot.strip() if hot else "",
                        "rank": i
                    })

            return {
                "platform": "å¾®åšçƒ­æœ",
                "news_list": items,
                "total": len(items),
                "source": "ç½‘é¡µçˆ¬å–"
            }

        except Exception as e:
            print(f"   âš ï¸ å¾®åšçˆ¬å–å¤±è´¥: {e}")
            return None

    async def scrape_baidu(self, page, limit=10):
        """çˆ¬å–ç™¾åº¦çƒ­æœ"""
        try:
            await page.goto("https://top.baidu.com/board?tab=realtime", timeout=15000)
            await page.wait_for_selector(".c-single-text-ellipsis", timeout=10000)

            items = []
            elements = await page.query_selector_all(".category-wrap_iQLoo.vertical_3uCeJ_0")

            for i, el in enumerate(elements[:limit], 1):
                # æ ‡é¢˜
                title_el = await el.query_selector(".c-single-text-ellipsis")
                if title_el:
                    title = await title_el.inner_text()

                    # é“¾æ¥å’Œçƒ­åº¦
                    link_el = await el.query_selector("a")
                    link = ""
                    if link_el:
                        link = await page.evaluate("(el) => el.href", link_el)

                    hot_el = await el.query_selector(".hot-index_1Bl1a")
                    hot = ""
                    if hot_el:
                        hot = await hot_el.inner_text()

                    items.append({
                        "title": title.strip(),
                        "url": link,
                        "hot": hot.strip() if hot else "",
                        "rank": i
                    })

            return {
                "platform": "ç™¾åº¦çƒ­æœ",
                "news_list": items,
                "total": len(items),
                "source": "ç½‘é¡µçˆ¬å–"
            }

        except Exception as e:
            print(f"   âš ï¸ ç™¾åº¦çˆ¬å–å¤±è´¥: {e}")
            return None

    def format_output(self, result):
        """æ ¼å¼åŒ–è¾“å‡º"""
        if not result:
            return ""

        lines = []
        lines.append(f"\n{'='*60}")
        lines.append(f"ğŸ“Š {result['platform']}")
        lines.append(f"{'='*60}")
        lines.append(f"ğŸ“¦ æ¥æº: {result['source']}")
        lines.append(f"ğŸ“Š æ•°é‡: {result['total']} æ¡")
        lines.append(f"â° æ—¶é—´: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
        lines.append("")

        for item in result['news_list']:
            title = item.get('title', 'N/A')
            hot = item.get('hot', '')
            url = item.get('url', '')

            lines.append(f"{item['rank']}. {title}")
            if hot:
                lines.append(f"   ğŸ”¥ çƒ­åº¦: {hot}")
            if url:
                lines.append(f"   ğŸ”— {url}")
            lines.append("")

        return "\n".join(lines)


    async def scrape_batch(self, platforms: List[str], limit: int = 10) -> List[Dict]:
        """æ‰¹é‡çˆ¬å–å¤šä¸ªå¹³å°

        Args:
            platforms: å¹³å°åˆ—è¡¨ï¼Œå¦‚ ["weibo", "zhihu", "baidu"]
            limit: æ¯ä¸ªå¹³å°è·å–æ•°é‡

        Returns:
            æ‰€æœ‰å¹³å°çš„ç»“æœåˆ—è¡¨
        """
        results = []

        async with async_playwright() as p:
            browser = await p.chromium.launch(headless=True)
            page = await browser.new_page()

            for platform in platforms:
                if platform not in self.supported_platforms:
                    print(f"âš ï¸ ä¸æ”¯æŒçš„å¹³å°: {platform}")
                    continue

                platform_info = self.supported_platforms[platform]
                func_name = platform_info["func"]

                print(f"ğŸ“° æ­£åœ¨çˆ¬å– {platform_info['name']}...")

                # åŠ¨æ€è°ƒç”¨å¯¹åº”çš„æ–¹æ³•
                scrape_func = getattr(self, func_name)
                result = await scrape_func(page, limit=limit)

                if result:
                    results.append(result)
                    print(f"   âœ… æˆåŠŸè·å– {result['total']} æ¡")
                else:
                    print(f"   âŒ çˆ¬å–å¤±è´¥")

            await browser.close()

        return results

    def print_results(self, results: List[Dict]):
        """æ‰“å°æ‰€æœ‰ç»“æœ"""
        for result in results:
            print(self.format_output(result))


def main():
    """å‘½ä»¤è¡Œå…¥å£"""
    import argparse

    parser = argparse.ArgumentParser(description="çƒ­æ¦œçˆ¬å–å™¨ (Playwright)")
    parser.add_argument(
        "-p", "--platforms",
        nargs="+",
        default=["weibo"],
        choices=["weibo", "zhihu", "baidu"],
        help="è¦çˆ¬å–çš„å¹³å°"
    )
    parser.add_argument(
        "-n", "--num",
        type=int,
        default=10,
        help="æ¯ä¸ªå¹³å°è·å–çš„æ•°é‡"
    )

    args = parser.parse_args()

    # è¿è¡Œ
    async def run():
        print("="*60)
        print("ğŸ” çƒ­æ¦œçˆ¬å–å™¨ (Playwright)")
        print("="*60)
        print()

        scraper = NewsScraper()
        results = await scraper.scrape_batch(args.platforms, args.num)
        scraper.print_results(results)

        print("\nâœ… å®Œæˆ!")

    asyncio.run(run())


if __name__ == "__main__":
    main()
