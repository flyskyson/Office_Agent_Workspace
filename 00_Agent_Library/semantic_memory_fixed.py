#!/usr/bin/env python3
# -*- coding: utf-8 -*-

# ============================================================================
# SSLè¯ä¹¦é—®é¢˜ä¿®å¤
# ============================================================================
import os
# ä½¿ç”¨HF-Mirroré•œåƒè§£å†³SSLé—®é¢˜
os.environ['HF_ENDPOINT'] = 'https://hf-mirror.com'

# å¦‚æœä»æœ‰é—®é¢˜ï¼Œå¯ä»¥ç¦ç”¨SSLéªŒè¯ï¼ˆä»…å¼€å‘ç¯å¢ƒï¼‰
# import ssl
# import urllib3
# urllib3.disable_warnings(urllib3.exceptions.InsecureRequestWarning)
# ssl._create_default_https_context = ssl._create_unverified_context


"""
å‘é‡è¯­ä¹‰è®°å¿†ç³»ç»Ÿ

åŸºäºChromaDBå’Œsentence-transformersçš„è¯­ä¹‰æœç´¢å®ç°ï¼Œ
ä¸ºClaude Codeæä¾›çœŸæ­£çš„è¯­ä¹‰ç†è§£èƒ½åŠ›ã€‚

ä½œè€…: Claude Code
æ—¥æœŸ: 2026-01-16
ç‰ˆæœ¬: v2.0
"""

import sys
from pathlib import Path
from typing import List, Dict, Any, Optional
from datetime import datetime
import json

# Windows ç»ˆç«¯ç¼–ç ä¿®å¤
if sys.platform == 'win32':
    try:
        import codecs
        sys.stdout = codecs.getwriter('utf-8')(sys.stdout.buffer, 'strict')
        sys.stderr = codecs.getwriter('utf-8')(sys.stderr.buffer, 'strict')
    except:
        pass

# ============================================================================
# å¯¼å…¥ä¾èµ–ï¼ˆå»¶è¿Ÿå¯¼å…¥ä»¥æä¾›å‹å¥½çš„é”™è¯¯ä¿¡æ¯ï¼‰
# ============================================================================

def check_dependencies():
    """æ£€æŸ¥å¿…è¦çš„ä¾èµ–æ˜¯å¦å®‰è£…"""
    missing = []

    try:
        import chromadb
    except ImportError:
        missing.append('chromadb')

    try:
        from sentence_transformers import SentenceTransformer
    except ImportError:
        missing.append('sentence-transformers')

    return missing


# ============================================================================
# è¯­ä¹‰è®°å¿†æ ¸å¿ƒç±»
# ============================================================================

class SemanticMemory:
    """
    å‘é‡è¯­ä¹‰è®°å¿†ç³»ç»Ÿ

    åŸºäºChromaDBå’Œsentence-transformerså®ç°é«˜æ€§èƒ½è¯­ä¹‰æœç´¢ã€‚

    ç‰¹æ€§:
    - çœŸæ­£çš„è¯­ä¹‰ç†è§£ï¼ˆéå…³é”®è¯åŒ¹é…ï¼‰
    - ä¸­è‹±æ–‡æ··åˆæ”¯æŒ
    - äºšæ¯«ç§’çº§æœç´¢é€Ÿåº¦
    - è‡ªåŠ¨å¢é‡æ›´æ–°
    - å…ƒæ•°æ®è¿‡æ»¤
    """

    # æ¨èçš„ä¸­æ–‡åµŒå…¥æ¨¡å‹
    RECOMMENDED_MODELS = {
        'fast': 'paraphrase-multilingual-MiniLM-L12-v2',  # å¿«é€Ÿï¼Œé€‚åˆå¤§å¤šæ•°åœºæ™¯
        'quality': 'paraphrase-multilingual-mpnet-base-v2',  # é«˜è´¨é‡
        'large': 'moka-ai/m3e-large',  # ä¸­æ–‡ä¸“ç”¨å¤§æ¨¡å‹
    }

    def __init__(self,
                 workspace_root: Optional[Path] = None,
                 model_name: str = 'fast',
                 collection_name: str = 'claude_memories'):
        """
        åˆå§‹åŒ–è¯­ä¹‰è®°å¿†ç³»ç»Ÿ

        å‚æ•°:
            workspace_root: å·¥ä½œåŒºæ ¹ç›®å½•
            model_name: åµŒå…¥æ¨¡å‹åç§° ('fast', 'quality', 'large' æˆ–å…·ä½“æ¨¡å‹å)
            collection_name: ChromaDBé›†åˆåç§°
        """
        # æ£€æŸ¥ä¾èµ–
        missing = check_dependencies()
        if missing:
            print(f"âŒ ç¼ºå°‘ä¾èµ–: {', '.join(missing)}")
            print("ğŸ“¦ è¯·è¿è¡Œ: pip install", ' '.join(missing))
            raise ImportError(f"Missing dependencies: {missing}")

        # åˆå§‹åŒ–å·¥ä½œåŒº
        if workspace_root is None:
            workspace_root = Path(__file__).parent.parent

        self.workspace_root = Path(workspace_root)
        self.vector_db_dir = self.workspace_root / "06_Learning_Journal" / "vector_db"

        # ç¡®ä¿ç›®å½•å­˜åœ¨
        self.vector_db_dir.mkdir(parents=True, exist_ok=True)

        # åˆå§‹åŒ–åµŒå…¥æ¨¡å‹
        self._init_embedder(model_name)

        # åˆå§‹åŒ–ChromaDB
        self._init_chroma(collection_name)

        # ç»Ÿè®¡ä¿¡æ¯
        self.stats = {
            'total_memories': 0,
            'last_update': None,
            'model_name': self.model_name
        }

    def _init_embedder(self, model_name: str):
        """åˆå§‹åŒ–åµŒå…¥æ¨¡å‹"""
        from sentence_transformers import SentenceTransformer

        # è§£ææ¨¡å‹åç§°
        if model_name in self.RECOMMENDED_MODELS:
            self.model_name = self.RECOMMENDED_MODELS[model_name]
        else:
            self.model_name = model_name

        print(f"ğŸ”„ åŠ è½½åµŒå…¥æ¨¡å‹: {self.model_name}")
        self.embedder = SentenceTransformer(self.model_name)
        print(f"âœ… æ¨¡å‹åŠ è½½å®Œæˆ")

        # è®°å½•æ¨¡å‹ç»´åº¦
        self.embedding_dim = self.embedder.get_sentence_embedding_dimension()
        print(f"ğŸ“Š åµŒå…¥ç»´åº¦: {self.embedding_dim}")

    def _init_chroma(self, collection_name: str):
        """åˆå§‹åŒ–ChromaDB"""
        import chromadb

        # åˆ›å»ºæŒä¹…åŒ–å®¢æˆ·ç«¯
        print(f"ğŸ”„ åˆå§‹åŒ–å‘é‡æ•°æ®åº“: {self.vector_db_dir}")
        self.chroma_client = chromadb.PersistentClient(
            path=str(self.vector_db_dir)
        )

        # è·å–æˆ–åˆ›å»ºé›†åˆ
        try:
            self.collection = self.chroma_client.get_collection(name=collection_name)
            print(f"âœ… åŠ è½½ç°æœ‰é›†åˆ: {collection_name}")
            print(f"ğŸ“Š ç°æœ‰è®°å¿†æ•°: {self.collection.count()}")
        except:
            self.collection = self.chroma_client.create_collection(
                name=collection_name,
                metadata={"hnsw:space": "cosine"}  # ä½¿ç”¨ä½™å¼¦ç›¸ä¼¼åº¦
            )
            print(f"âœ… åˆ›å»ºæ–°é›†åˆ: {collection_name}")

    def add_memory(self,
                   memory_id: str,
                   text: str,
                   metadata: Optional[Dict[str, Any]] = None) -> bool:
        """
        æ·»åŠ è®°å¿†åˆ°å‘é‡æ•°æ®åº“

        å‚æ•°:
            memory_id: è®°å¿†å”¯ä¸€ID
            text: è®°å¿†æ–‡æœ¬å†…å®¹
            metadata: é™„åŠ å…ƒæ•°æ®

        è¿”å›:
            æ˜¯å¦æˆåŠŸ
        """
        try:
            # ç”ŸæˆåµŒå…¥å‘é‡
            embedding = self.embedder.encode(text, convert_to_numpy=True).tolist()

            # å‡†å¤‡å…ƒæ•°æ®
            if metadata is None:
                metadata = {}

            # æ·»åŠ æ—¶é—´æˆ³
            metadata['timestamp'] = datetime.now().isoformat()
            metadata['text_length'] = len(text)

            # è½¬æ¢å…ƒæ•°æ®å€¼ä¸ºå­—ç¬¦ä¸²ï¼ˆChromaDBè¦æ±‚ï¼‰
            metadata_str = {k: str(v) for k, v in metadata.items()}

            # æ·»åŠ åˆ°é›†åˆ
            self.collection.add(
                ids=[memory_id],
                embeddings=[embedding],
                documents=[text],
                metadatas=[metadata_str]
            )

            # æ›´æ–°ç»Ÿè®¡
            self.stats['total_memories'] = self.collection.count()
            self.stats['last_update'] = datetime.now().isoformat()

            return True

        except Exception as e:
            print(f"âŒ æ·»åŠ è®°å¿†å¤±è´¥: {e}")
            return False

    def add_memories_batch(self,
                          memories: List[Dict[str, Any]]) -> Dict[str, Any]:
        """
        æ‰¹é‡æ·»åŠ è®°å¿†ï¼ˆæ›´é«˜æ•ˆï¼‰

        å‚æ•°:
            memories: è®°å¿†åˆ—è¡¨ï¼Œæ¯ä¸ªè®°å¿†åŒ…å« id, text, metadata

        è¿”å›:
            æ‰¹é‡æ“ä½œç»“æœç»Ÿè®¡
        """
        results = {
            'success': 0,
            'failed': 0,
            'errors': []
        }

        try:
            # æ‰¹é‡ç”ŸæˆåµŒå…¥
            texts = [m['text'] for m in memories]
            embeddings = self.embedder.encode(texts, convert_to_numpy=True).tolist()

            # å‡†å¤‡æ•°æ®
            ids = [m['id'] for m in memories]
            documents = texts
            metadatas = []

            for m in memories:
                metadata = m.get('metadata', {})
                metadata['timestamp'] = datetime.now().isoformat()
                metadata['text_length'] = len(m['text'])
                # è½¬æ¢ä¸ºå­—ç¬¦ä¸²
                metadatas.append({k: str(v) for k, v in metadata.items()})

            # æ‰¹é‡æ·»åŠ 
            self.collection.add(
                ids=ids,
                embeddings=embeddings,
                documents=documents,
                metadatas=metadatas
            )

            results['success'] = len(memories)

        except Exception as e:
            results['failed'] = len(memories)
            results['errors'].append(str(e))

        # æ›´æ–°ç»Ÿè®¡
        self.stats['total_memories'] = self.collection.count()
        self.stats['last_update'] = datetime.now().isoformat()

        return results

    def search(self,
              query: str,
              top_k: int = 5,
              filter_metadata: Optional[Dict[str, str]] = None) -> List[Dict[str, Any]]:
        """
        è¯­ä¹‰æœç´¢

        å‚æ•°:
            query: æœç´¢æŸ¥è¯¢
            top_k: è¿”å›å‰Kä¸ªç»“æœ
            filter_metadata: å…ƒæ•°æ®è¿‡æ»¤æ¡ä»¶

        è¿”å›:
            æœç´¢ç»“æœåˆ—è¡¨ï¼Œæ¯ä¸ªç»“æœåŒ…å«:
            - id: è®°å¿†ID
            - text: è®°å¿†æ–‡æœ¬
            - metadata: å…ƒæ•°æ®
            - score: ç›¸ä¼¼åº¦åˆ†æ•° (0-1)
        """
        try:
            # ç”ŸæˆæŸ¥è¯¢åµŒå…¥
            query_embedding = self.embedder.encode(query, convert_to_numpy=True).tolist()

            # æ‰§è¡Œæœç´¢
            results = self.collection.query(
                query_embeddings=[query_embedding],
                n_results=top_k,
                where=filter_metadata
            )

            # æ ¼å¼åŒ–ç»“æœ
            formatted_results = []

            if results['ids'] and len(results['ids']) > 0:
                for i, memory_id in enumerate(results['ids'][0]):
                    # ChromaDBè¿”å›çš„è·ç¦»ï¼Œè½¬æ¢ä¸ºç›¸ä¼¼åº¦ (cosine distance -> similarity)
                    distance = results['distances'][0][i]
                    similarity = 1 - distance  # ä½™å¼¦è·ç¦»è½¬ç›¸ä¼¼åº¦

                    formatted_results.append({
                        'id': memory_id,
                        'text': results['documents'][0][i],
                        'metadata': results['metadatas'][0][i],
                        'similarity_score': round(similarity, 4),
                        'distance': round(distance, 4)
                    })

            return formatted_results

        except Exception as e:
            print(f"âŒ æœç´¢å¤±è´¥: {e}")
            return []

    def hybrid_search(self,
                     query: str,
                     keyword_results: List[Dict[str, Any]],
                     top_k: int = 5,
                     semantic_weight: float = 0.7) -> List[Dict[str, Any]]:
        """
        æ··åˆæœç´¢ï¼ˆè¯­ä¹‰+å…³é”®è¯ï¼‰

        ç»“åˆè¯­ä¹‰æœç´¢å’Œå…³é”®è¯æœç´¢çš„ç»“æœï¼Œ
       é€šè¿‡åŠ æƒèåˆè·å¾—æ›´å‡†ç¡®çš„ç»“æœã€‚

        å‚æ•°:
            query: æœç´¢æŸ¥è¯¢
            keyword_results: å…³é”®è¯æœç´¢ç»“æœ
            top_k: è¿”å›å‰Kä¸ªç»“æœ
            semantic_weight: è¯­ä¹‰æœç´¢æƒé‡ (0-1)

        è¿”å›:
            èåˆåçš„æœç´¢ç»“æœ
        """
        # è¯­ä¹‰æœç´¢
        semantic_results = self.search(query, top_k=top_k * 2)

        # åˆ›å»ºIDåˆ°ç»“æœçš„æ˜ å°„
        result_map = {}

        # æ·»åŠ è¯­ä¹‰ç»“æœ
        for result in semantic_results:
            result_map[result['id']] = {
                'result': result,
                'semantic_score': result['similarity_score'],
                'keyword_score': 0.0
            }

        # èåˆå…³é”®è¯ç»“æœ
        for i, kw_result in enumerate(keyword_results):
            result_id = kw_result.get('id') or kw_result.get('timestamp', '')

            if result_id in result_map:
                # å·²å­˜åœ¨ï¼Œæ›´æ–°å…³é”®è¯åˆ†æ•°
                # å…³é”®è¯åˆ†æ•°ï¼šä½ç½®è¶Šé å‰åˆ†æ•°è¶Šé«˜
                kw_score = 1.0 - (i / len(keyword_results))
                result_map[result_id]['keyword_score'] = kw_score
            else:
                # ä¸å­˜åœ¨ï¼Œæ·»åŠ æ–°ç»“æœ
                kw_score = 1.0 - (i / len(keyword_results))
                result_map[result_id] = {
                    'result': kw_result,
                    'semantic_score': 0.0,
                    'keyword_score': kw_score
                }

        # è®¡ç®—èåˆåˆ†æ•°
        for result_id, data in result_map.items():
            data['hybrid_score'] = (
                data['semantic_score'] * semantic_weight +
                data['keyword_score'] * (1 - semantic_weight)
            )

        # æ’åºå¹¶è¿”å›top-k
        sorted_results = sorted(
            result_map.values(),
            key=lambda x: x['hybrid_score'],
            reverse=True
        )[:top_k]

        # æ·»åŠ åˆ†æ•°ä¿¡æ¯åˆ°ç»“æœä¸­
        final_results = []
        for data in sorted_results:
            result = data['result'].copy()
            result['scores'] = {
                'semantic': round(data['semantic_score'], 4),
                'keyword': round(data['keyword_score'], 4),
                'hybrid': round(data['hybrid_score'], 4)
            }
            final_results.append(result)

        return final_results

    def delete_memory(self, memory_id: str) -> bool:
        """åˆ é™¤è®°å¿†"""
        try:
            self.collection.delete(ids=[memory_id])
            self.stats['total_memories'] = self.collection.count()
            return True
        except Exception as e:
            print(f"âŒ åˆ é™¤å¤±è´¥: {e}")
            return False

    def update_memory(self,
                     memory_id: str,
                     text: Optional[str] = None,
                     metadata: Optional[Dict[str, Any]] = None) -> bool:
        """æ›´æ–°è®°å¿†"""
        try:
            # ChromaDBä¸æ”¯æŒç›´æ¥æ›´æ–°ï¼Œéœ€è¦å…ˆåˆ é™¤å†æ·»åŠ 
            self.delete_memory(memory_id)

            # è·å–åŸå§‹æ•°æ®ï¼ˆå¦‚æœåªæ›´æ–°éƒ¨åˆ†å­—æ®µï¼‰
            if text is None or metadata is None:
                # è¿™é‡Œéœ€è¦ä»å¤‡ä»½å­˜å‚¨ä¸­è·å–åŸå§‹æ•°æ®
                # æš‚æ—¶è¦æ±‚æä¾›å®Œæ•´æ•°æ®
                raise ValueError("æ›´æ–°éœ€è¦æä¾›å®Œæ•´çš„textå’Œmetadata")

            # é‡æ–°æ·»åŠ 
            return self.add_memory(memory_id, text, metadata)

        except Exception as e:
            print(f"âŒ æ›´æ–°å¤±è´¥: {e}")
            return False

    def get_stats(self) -> Dict[str, Any]:
        """è·å–ç»Ÿè®¡ä¿¡æ¯"""
        self.stats['total_memories'] = self.collection.count()
        return self.stats.copy()

    def clear_all(self) -> bool:
        """æ¸…ç©ºæ‰€æœ‰è®°å¿†ï¼ˆå±é™©æ“ä½œï¼‰"""
        try:
            self.chroma_client.delete_collection(self.collection.name)
            self._init_chroma(self.collection.name)
            self.stats['total_memories'] = 0
            return True
        except Exception as e:
            print(f"âŒ æ¸…ç©ºå¤±è´¥: {e}")
            return False

    def print_stats(self):
        """æ‰“å°ç»Ÿè®¡ä¿¡æ¯"""
        stats = self.get_stats()

        print("\n" + "=" * 70)
        print("ğŸ§  è¯­ä¹‰è®°å¿†ç³»ç»Ÿç»Ÿè®¡")
        print("=" * 70)
        print(f"\nğŸ“Š è®°å¿†æ•°é‡: {stats['total_memories']}")
        print(f"ğŸ¤– æ¨¡å‹: {stats['model_name']}")
        print(f"ğŸ“ åµŒå…¥ç»´åº¦: {self.embedding_dim}")
        print(f"ğŸ• æœ€åæ›´æ–°: {stats['last_update']}")
        print(f"ğŸ“ æ•°æ®åº“è·¯å¾„: {self.vector_db_dir}")
        print("\n" + "=" * 70)


# ============================================================================
# è®°å¿†è¿ç§»å·¥å…·
# ============================================================================

class MemoryMigrator:
    """
    è®°å¿†è¿ç§»å·¥å…·

    å°†ç°æœ‰çš„JSONè®°å¿†è¿ç§»åˆ°å‘é‡æ•°æ®åº“
    """

    def __init__(self, semantic_memory: SemanticMemory):
        self.semantic_memory = semantic_memory

    def migrate_from_json(self,
                         json_file: Path,
                         batch_size: int = 10) -> Dict[str, Any]:
        """
        ä»JSONæ–‡ä»¶è¿ç§»è®°å¿†

        å‚æ•°:
            json_file: JSONè®°å¿†æ–‡ä»¶è·¯å¾„
            batch_size: æ‰¹é‡å¤„ç†å¤§å°

        è¿”å›:
            è¿ç§»ç»“æœç»Ÿè®¡
        """
        if not json_file.exists():
            return {'error': 'æ–‡ä»¶ä¸å­˜åœ¨'}

        # è¯»å–JSON
        with open(json_file, 'r', encoding='utf-8') as f:
            data = json.load(f)

        # æå–contexts
        contexts = data.get('contexts', [])

        if not contexts:
            return {'error': 'æ²¡æœ‰æ‰¾åˆ°contexts'}

        # æ‰¹é‡è¿ç§»
        results = {
            'total': len(contexts),
            'success': 0,
            'failed': 0
        }

        for i in range(0, len(contexts), batch_size):
            batch = contexts[i:i + batch_size]

            # å‡†å¤‡è®°å¿†æ•°æ®
            memories = []
            for ctx in batch:
                # ç»„åˆæ–‡æœ¬ï¼ˆä¸»é¢˜+æ‘˜è¦ï¼‰
                text = f"{ctx.get('topic', '')}. {ctx.get('summary', '')}"

                # å‡†å¤‡å…ƒæ•°æ®
                metadata = {
                    'topic': ctx.get('topic', ''),
                    'session_id': ctx.get('session_id', ''),
                    'priority': ctx.get('priority', 'normal'),
                    'tags': ','.join(ctx.get('tags', [])),
                    'timestamp': ctx.get('timestamp', '')
                }

                memories.append({
                    'id': ctx.get('timestamp', str(hash(text))),
                    'text': text,
                    'metadata': metadata
                })

            # æ‰¹é‡æ·»åŠ 
            batch_result = self.semantic_memory.add_memories_batch(memories)
            results['success'] += batch_result['success']
            results['failed'] += batch_result['failed']

            print(f"ğŸ”„ è¿›åº¦: {min(i + batch_size, len(contexts))}/{len(contexts)}")

        return results


# ============================================================================
# æ¼”ç¤ºç¨‹åº
# ============================================================================

def demo_semantic_memory():
    """æ¼”ç¤ºè¯­ä¹‰è®°å¿†ç³»ç»Ÿ"""

    print("""
â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—
â•‘                                                                    â•‘
â•‘              å‘é‡è¯­ä¹‰è®°å¿†ç³»ç»Ÿæ¼”ç¤º (v2.0)                            â•‘
â•‘                                                                    â•‘
â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    """)

    # åˆ›å»ºè¯­ä¹‰è®°å¿†
    semantic = SemanticMemory()

    # æ·»åŠ ç¤ºä¾‹è®°å¿†
    print("\nğŸ“ æ·»åŠ ç¤ºä¾‹è®°å¿†...")
    print("-" * 70)

    memories = [
        {
            'id': 'mem_001',
            'text': 'å¤šAgentç³»ç»Ÿå¼€å‘ï¼šåˆ›å»ºäº†åŸºäºWorkflowEngineçš„å¤šAgentæ¼”ç¤ºç³»ç»Ÿ',
            'metadata': {'topic': 'å¤šAgentç³»ç»Ÿ', 'priority': 'high'}
        },
        {
            'id': 'mem_002',
            'text': 'å¸‚åœºç›‘ç®¡æ™ºèƒ½ä½“ï¼šä½¿ç”¨Jinja2æ¨¡æ¿å¼•æ“ç”Ÿæˆä¸ªä½“å·¥å•†æˆ·å¼€ä¸šç”³è¯·ä¹¦',
            'metadata': {'topic': 'å¸‚åœºç›‘ç®¡', 'priority': 'normal'}
        },
        {
            'id': 'mem_003',
            'text': 'Claude Codeæ ¸å¿ƒè§’è‰²å®šä¹‰ï¼šä¸åªæ˜¯ä¼šç”¨å·¥å…·çš„AIï¼Œè€Œæ˜¯æœ‰è®°å¿†ã€èƒ½æ€è€ƒã€ä¼šè¿›åŒ–çš„åä½œä¼™ä¼´',
            'metadata': {'topic': 'è§’è‰²å®šä¹‰', 'priority': 'high'}
        },
        {
            'id': 'mem_004',
            'text': 'å®ç°è‡ªåŠ¨è®°å¿†åŠ è½½ç³»ç»Ÿï¼šåˆ›å»ºsession_initializer.pyï¼Œå®ç°æ¯æ¬¡ä¼šè¯å¼€å§‹æ—¶è‡ªåŠ¨åŠ è½½è§’è‰²å®šä¹‰',
            'metadata': {'topic': 'è®°å¿†ç³»ç»Ÿ', 'priority': 'high'}
        },
        {
            'id': 'mem_005',
            'text': 'æ—¥å¸¸ç»´æŠ¤ä»»åŠ¡ï¼šæ¸…ç†ä¸´æ—¶æ–‡ä»¶ï¼Œæ•´ç†æ–‡æ¡£ç›®å½•',
            'metadata': {'topic': 'æ—¥å¸¸ä»»åŠ¡', 'priority': 'low'}
        }
    ]

    result = semantic.add_memories_batch(memories)
    print(f"âœ… æ‰¹é‡æ·»åŠ å®Œæˆ: {result['success']} æˆåŠŸ, {result['failed']} å¤±è´¥")

    # è¯­ä¹‰æœç´¢æ¼”ç¤º
    print("\nğŸ” è¯­ä¹‰æœç´¢æ¼”ç¤º")
    print("-" * 70)

    queries = [
        "Agentç›¸å…³çš„",
        "å¦‚ä½•ç”Ÿæˆç”³è¯·ä¹¦",
        "è®°å¿†å’Œè§’è‰²",
        "ç³»ç»Ÿç»´æŠ¤"
    ]

    for query in queries:
        print(f"\nğŸ’­ æŸ¥è¯¢: {query}")
        results = semantic.search(query, top_k=2)

        for i, result in enumerate(results, 1):
            print(f"\n   ç»“æœ {i}:")
            print(f"   ğŸ“„ å†…å®¹: {result['text'][:80]}...")
            print(f"   ğŸ¯ ç›¸ä¼¼åº¦: {result['similarity_score']:.2%}")
            print(f"   ğŸ“Œ ä¸»é¢˜: {result['metadata'].get('topic', 'N/A')}")

    # æ˜¾ç¤ºç»Ÿè®¡
    semantic.print_stats()

    print("\nâœ… æ¼”ç¤ºå®Œæˆï¼")


if __name__ == "__main__":
    demo_semantic_memory()
