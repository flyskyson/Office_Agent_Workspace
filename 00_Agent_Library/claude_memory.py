#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
Claude Code è®°å¿†æŒä¹…åŒ–ç³»ç»Ÿ

è®©Claude Codeæ‹¥æœ‰è·¨ä¼šè¯çš„æŒä¹…åŒ–è®°å¿†èƒ½åŠ›ï¼š
1. ä¸Šä¸‹æ–‡è®°å¿† - è®°ä½æ¯æ¬¡å¯¹è¯çš„ä¸Šä¸‹æ–‡
2. å†³ç­–è®°å¿† - è®°ä½å·¥å…·é€‰æ‹©å’Œå†³ç­–é€»è¾‘
3. ç”¨æˆ·åå¥½ - è®°ä½ç”¨æˆ·çš„ä¹ æƒ¯å’Œåå¥½
4. é¡¹ç›®çŸ¥è¯† - è®°ä½é¡¹ç›®ç‰¹å®šçš„çŸ¥è¯†
5. æ¼”è¿›è½¨è¿¹ - è®°ä½ç³»ç»Ÿå’Œé¡¹ç›®çš„æ¼”è¿›

ä½œè€…: Claude Code
æ—¥æœŸ: 2026-01-15
"""

import sys
import json
from pathlib import Path
from datetime import datetime, timedelta
from typing import Dict, List, Any, Optional
from collections import defaultdict
import re

# Windows ç»ˆç«¯ç¼–ç ä¿®å¤
if sys.platform == 'win32':
    try:
        import codecs
        sys.stdout = codecs.getwriter('utf-8')(sys.stdout.buffer, 'strict')
        sys.stderr = codecs.getwriter('utf-8')(sys.stderr.buffer, 'strict')
    except:
        pass


# ============================================================================
# LangMem é£æ ¼å¢å¼ºåŠŸèƒ½
# ============================================================================

class ImportanceScorer:
    """
    é‡è¦æ€§è¯„åˆ†å™¨ - LangMem é£æ ¼

    ä¸ºè®°å¿†è®¡ç®—é‡è¦æ€§åˆ†æ•° (0-100)ï¼ŒåŸºäº:
    - å…³é”®è¯åŒ¹é…åº¦
    - å†…å®¹é•¿åº¦å’Œè´¨é‡
    - æ—¶é—´æ–°é²œåº¦
    - äº¤äº’é¢‘ç‡
    """

    # é«˜æƒé‡å…³é”®è¯ï¼ˆç”¨æˆ·å…´è¶£ï¼‰
    HIGH_WEIGHT_KEYWORDS = [
        # æŠ€æœ¯æ ˆ
        "LangGraph", "å¤šAgent", "WorkflowEngine", "å·¥ä½œæµå¼•æ“",
        "Playwright", "Jinja2", "ChromaDB", "Streamlit", "Flask",
        # æ ¸å¿ƒæ¦‚å¿µ
        "è®°å¿†ç³»ç»Ÿ", "æ£€æŸ¥ç‚¹", "çŠ¶æ€ç®¡ç†", "å¯è§†åŒ–",
        "Supervisor", "Coordinator", "æ™ºèƒ½ä½“",
        # é¡¹ç›®ç›¸å…³
        "å¸‚åœºç›‘ç®¡", "è®°å¿†åŠ©æ‰‹", "ç”³è¯·ä¹¦ç”Ÿæˆ",
        # å¼€å‘ç›¸å…³
        "æ¶æ„è®¾è®¡", "æœ€ä½³å®è·µ", "ç³»ç»Ÿæ¼”è¿›",
        "æ•ˆç‡ç›‘æ§", "è‡ªåŠ¨åŒ–", "å·¥å…·é›†æˆ"
    ]

    # ä¸­ç­‰æƒé‡å…³é”®è¯
    MEDIUM_WEIGHT_KEYWORDS = [
        "Python", "JavaScript", "API", "æ•°æ®åº“",
        "å‰ç«¯", "åç«¯", "éƒ¨ç½²", "æµ‹è¯•",
        "æ–‡æ¡£", "æ•™ç¨‹", "ç¤ºä¾‹"
    ]

    def __init__(self):
        self.keyword_weights = self._build_keyword_weights()

    def _build_keyword_weights(self) -> Dict[str, float]:
        """æ„å»ºå…³é”®è¯æƒé‡å­—å…¸"""
        weights = {}
        for kw in self.HIGH_WEIGHT_KEYWORDS:
            weights[kw.lower()] = 3.0  # é«˜æƒé‡
        for kw in self.MEDIUM_WEIGHT_KEYWORDS:
            weights[kw.lower()] = 1.5  # ä¸­æƒé‡
        return weights

    def calculate(self, memory: Dict[str, Any]) -> float:
        """
        è®¡ç®—è®°å¿†çš„é‡è¦æ€§åˆ†æ•°

        å‚æ•°:
            memory: è®°å¿†å­—å…¸ï¼ˆåŒ…å« topic, summary, key_points ç­‰ï¼‰

        è¿”å›:
            é‡è¦æ€§åˆ†æ•° (0-100)
        """
        score = 0.0

        # 1. å…³é”®è¯åŒ¹é…åº¦ (0-40åˆ†)
        keyword_score = self._calculate_keyword_score(memory)
        score += keyword_score

        # 2. å†…å®¹è´¨é‡ (0-25åˆ†)
        quality_score = self._calculate_quality_score(memory)
        score += quality_score

        # 3. æ—¶é—´æ–°é²œåº¦ (0-20åˆ†)
        recency_score = self._calculate_recency_score(memory)
        score += recency_score

        # 4. ä¼˜å…ˆçº§ (0-15åˆ†)
        priority_score = self._calculate_priority_score(memory)
        score += priority_score

        return min(score, 100.0)

    def _calculate_keyword_score(self, memory: Dict[str, Any]) -> float:
        """è®¡ç®—å…³é”®è¯åŒ¹é…åˆ†æ•°"""
        # åˆå¹¶æ‰€æœ‰æ–‡æœ¬å­—æ®µ
        text = ' '.join([
            memory.get('topic', ''),
            memory.get('summary', ''),
            ' '.join(memory.get('key_points', [])),
            ' '.join(memory.get('decisions_made', [])),
            memory.get('outcomes', '')
        ]).lower()

        # è®¡ç®—åŒ¹é…æƒé‡
        total_weight = 0.0
        for keyword, weight in self.keyword_weights.items():
            # ä½¿ç”¨æ­£åˆ™åŒ¹é…å®Œæ•´å•è¯
            pattern = r'\b' + re.escape(keyword) + r'\b'
            if re.search(pattern, text):
                total_weight += weight

        # è½¬æ¢ä¸º0-40åˆ†
        return min(total_weight * 5, 40.0)

    def _calculate_quality_score(self, memory: Dict[str, Any]) -> float:
        """è®¡ç®—å†…å®¹è´¨é‡åˆ†æ•°"""
        score = 0.0

        # é•¿åº¦åˆ†æ•° (0-10åˆ†)
        summary_len = len(memory.get('summary', ''))
        key_points_count = len(memory.get('key_points', []))

        if summary_len > 50:
            score += 5
        if summary_len > 150:
            score += 3
        if key_points_count >= 3:
            score += 2

        # ç»“æ„å®Œæ•´æ€§ (0-15åˆ†)
        required_fields = ['topic', 'summary', 'outcomes']
        for field in required_fields:
            if memory.get(field):
                score += 3

        if memory.get('key_points'):
            score += 3
        if memory.get('tools_used'):
            score += 3

        return min(score, 25.0)

    def _calculate_recency_score(self, memory: Dict[str, Any]) -> float:
        """è®¡ç®—æ—¶é—´æ–°é²œåº¦åˆ†æ•°"""
        timestamp_str = memory.get('timestamp', '')
        if not timestamp_str:
            return 0.0

        try:
            timestamp = datetime.fromisoformat(timestamp_str)
            age = datetime.now() - timestamp

            # è¶Šæ–°åˆ†æ•°è¶Šé«˜
            if age.days <= 1:
                return 20.0
            elif age.days <= 7:
                return 15.0
            elif age.days <= 30:
                return 10.0
            elif age.days <= 90:
                return 5.0
            else:
                return 2.0  # æ—§è®°å¿†ä»æœ‰ä¸€å®šä»·å€¼
        except:
            return 0.0

    def _calculate_priority_score(self, memory: Dict[str, Any]) -> float:
        """è®¡ç®—ä¼˜å…ˆçº§åˆ†æ•°"""
        priority = memory.get('priority', 'normal')

        if priority == 'high':
            return 15.0
        elif priority == 'normal':
            return 10.0
        elif priority == 'low':
            return 5.0
        else:
            return 10.0


class SemanticRetriever:
    """
    è¯­ä¹‰æ£€ç´¢å™¨ - LangMem é£æ ¼

    åŸºäºå…³é”®è¯åŒ¹é…çš„è½»é‡çº§è¯­ä¹‰æ£€ç´¢
    """

    def __init__(self, scorer: ImportanceScorer):
        self.scorer = scorer

    def search(self, memories: List[Dict[str, Any]], query: str,
               top_k: int = 5, min_score: float = 20.0) -> List[Dict[str, Any]]:
        """
        æœç´¢ç›¸å…³è®°å¿†

        å‚æ•°:
            memories: è®°å¿†åˆ—è¡¨
            query: æœç´¢æŸ¥è¯¢
            top_k: è¿”å›å‰Kä¸ªç»“æœ
            min_score: æœ€ä½ç›¸å…³æ€§åˆ†æ•°

        è¿”å›:
            æ’åºåçš„ç›¸å…³è®°å¿†åˆ—è¡¨
        """
        # è®¡ç®—æ¯æ¡è®°å¿†çš„ç›¸å…³æ€§åˆ†æ•°
        scored_memories = []
        for memory in memories:
            relevance = self._calculate_relevance(memory, query)
            if relevance >= min_score:
                scored_memories.append((memory, relevance))

        # æŒ‰ç›¸å…³æ€§æ’åº
        scored_memories.sort(key=lambda x: x[1], reverse=True)

        # è¿”å›top_kç»“æœï¼ˆé™„å¸¦ç›¸å…³æ€§åˆ†æ•°ï¼‰
        results = []
        for memory, score in scored_memories[:top_k]:
            memory_with_score = memory.copy()
            memory_with_score['_relevance_score'] = round(score, 2)
            results.append(memory_with_score)

        return results

    def _calculate_relevance(self, memory: Dict[str, Any], query: str) -> float:
        """
        è®¡ç®—è®°å¿†ä¸æŸ¥è¯¢çš„ç›¸å…³æ€§

        ç»¼åˆè€ƒè™‘:
        - æ–‡æœ¬åŒ¹é…åº¦
        - è®°å¿†æœ¬èº«çš„é‡è¦æ€§
        """
        # 1. æ–‡æœ¬åŒ¹é…åº¦ (0-50åˆ†)
        text_match = self._calculate_text_match(memory, query)

        # 2. è®°å¿†é‡è¦æ€§åŠ æƒ (0-50åˆ†)
        importance = self.scorer.calculate(memory) * 0.5

        return text_match + importance

    def _calculate_text_match(self, memory: Dict[str, Any], query: str) -> float:
        """è®¡ç®—æ–‡æœ¬åŒ¹é…åº¦"""
        query_lower = query.lower()

        # åˆå¹¶æ‰€æœ‰æ–‡æœ¬å­—æ®µ
        text = ' '.join([
            memory.get('topic', ''),
            memory.get('summary', ''),
            ' '.join(memory.get('key_points', [])),
            ' '.join(memory.get('decisions_made', [])),
            memory.get('outcomes', ''),
            ' '.join(memory.get('tags', []))
        ]).lower()

        # å®Œå…¨åŒ¹é…
        if query_lower in text:
            base_score = 30.0

            # æ£€æŸ¥åŒ¹é…ä½ç½®
            if query_lower in memory.get('topic', '').lower():
                base_score += 15.0  # ä¸»é¢˜åŒ¹é…æƒé‡æœ€é«˜
            elif query_lower in memory.get('summary', '').lower():
                base_score += 10.0  # æ‘˜è¦åŒ¹é…æ¬¡ä¹‹

            return base_score

        # éƒ¨åˆ†åŒ¹é…ï¼ˆå•è¯çº§åˆ«ï¼‰
        query_words = set(query_lower.split())
        text_words = set(text.split())

        overlap = query_words & text_words
        if overlap:
            return len(overlap) / len(query_words) * 30.0

        return 0.0


class MemoryCleaner:
    """
    è®°å¿†æ¸…ç†å™¨ - LangMem é£æ ¼

    è‡ªåŠ¨æ¸…ç†ä½ä»·å€¼è®°å¿†ï¼Œä¿æŒè®°å¿†åº“å¥åº·
    """

    def __init__(self, scorer: ImportanceScorer):
        self.scorer = scorer

    def cleanup_low_score(self, memories: List[Dict[str, Any]],
                         threshold: float = 30.0,
                         keep_recent_days: int = 30,
                         dry_run: bool = False) -> Dict[str, Any]:
        """
        æ¸…ç†ä½åˆ†è®°å¿†

        å‚æ•°:
            memories: è®°å¿†åˆ—è¡¨
            threshold: é‡è¦æ€§åˆ†æ•°é˜ˆå€¼ï¼ˆä½äºæ­¤åˆ†æ•°å°†è¢«æ¸…ç†ï¼‰
            keep_recent_days: ä¿ç•™æœ€è¿‘Nå¤©çš„è®°å¿†ï¼ˆä¸ç®¡åˆ†æ•°ï¼‰
            dry_run: ä»…æ¨¡æ‹Ÿï¼Œä¸å®é™…åˆ é™¤

        è¿”å›:
            æ¸…ç†ç»Ÿè®¡ä¿¡æ¯
        """
        cutoff_date = datetime.now() - timedelta(days=keep_recent_days)
        to_remove = []
        to_keep = []

        for memory in memories:
            timestamp_str = memory.get('timestamp', '')
            try:
                timestamp = datetime.fromisoformat(timestamp_str)
                is_recent = timestamp > cutoff_date
            except:
                is_recent = False

            score = self.scorer.calculate(memory)

            # å†³å®šæ˜¯å¦ä¿ç•™
            if is_recent or score >= threshold:
                to_keep.append(memory)
            else:
                to_remove.append(memory)

        if not dry_run:
            # å®é™…æ¸…ç†ï¼šåªä¿ç•™to_keep
            cleaned_memories = to_keep
        else:
            # æ¨¡æ‹Ÿï¼šä¿ç•™æ‰€æœ‰
            cleaned_memories = memories

        return {
            'original_count': len(memories),
            'removed_count': len(to_remove),
            'kept_count': len(to_keep),
            'removed_scores': [self.scorer.calculate(m) for m in to_remove],
            'dry_run': dry_run,
            'cleaned_memories': cleaned_memories if not dry_run else memories
        }

    def suggest_cleanup(self, memories: List[Dict[str, Any]]) -> Dict[str, Any]:
        """
        å»ºè®®æ¸…ç†ç­–ç•¥ï¼ˆä¸å®é™…æ‰§è¡Œï¼‰

        åˆ†æè®°å¿†åº“çŠ¶æ€ï¼Œæä¾›æ¸…ç†å»ºè®®
        """
        # ç»Ÿè®¡åˆ†æ•°åˆ†å¸ƒ
        scores = [self.scorer.calculate(m) for m in memories]

        if not scores:
            return {
                'total_memories': 0,
                'suggestion': 'æ— éœ€æ¸…ç†',
                'details': 'è®°å¿†åº“ä¸ºç©º'
            }

        avg_score = sum(scores) / len(scores)
        low_score_count = sum(1 for s in scores if s < 30)
        high_score_count = sum(1 for s in scores if s >= 70)

        # ç”Ÿæˆå»ºè®®
        if low_score_count > len(memories) * 0.3:
            suggestion = f"å»ºè®®æ¸…ç† {low_score_count} æ¡ä½åˆ†è®°å¿†ï¼ˆ<30åˆ†ï¼‰"
            threshold = 30.0
        elif low_score_count > len(memories) * 0.2:
            suggestion = f"å»ºè®®æ¸…ç† {low_score_count} æ¡ä½åˆ†è®°å¿†ï¼ˆ<30åˆ†ï¼‰"
            threshold = 30.0
        else:
            suggestion = "è®°å¿†åº“å¥åº·ï¼Œæ— éœ€æ¸…ç†"
            threshold = None

        return {
            'total_memories': len(memories),
            'average_score': round(avg_score, 2),
            'low_score_count': low_score_count,
            'high_score_count': high_score_count,
            'suggestion': suggestion,
            'suggested_threshold': threshold,
            'score_distribution': {
                'min': round(min(scores), 2),
                'max': round(max(scores), 2),
                'avg': round(avg_score, 2)
            }
        }


# ============================================================================
# è®°å¿†å­˜å‚¨
# ============================================================================

class MemoryStore:
    """è®°å¿†å­˜å‚¨ - æŒä¹…åŒ–Claude Codeçš„æ‰€æœ‰è®°å¿†"""

    def __init__(self, workspace_root: Path):
        self.workspace_root = Path(workspace_root)
        self.memory_dir = self.workspace_root / "06_Learning_Journal" / "claude_memory"

        # ç¡®ä¿ç›®å½•å­˜åœ¨
        self.memory_dir.mkdir(parents=True, exist_ok=True)

        # è®°å¿†æ–‡ä»¶
        self.files = {
            'contexts': self.memory_dir / "contexts.json",      # ä¸Šä¸‹æ–‡è®°å¿†
            'decisions': self.memory_dir / "decisions.json",    # å†³ç­–è®°å¿†
            'preferences': self.memory_dir / "preferences.json", # ç”¨æˆ·åå¥½
            'projects': self.memory_dir / "projects.json",      # é¡¹ç›®çŸ¥è¯†
            'evolution': self.memory_dir / "evolution.json",    # æ¼”è¿›è½¨è¿¹
            'conversations': self.memory_dir / "conversations.json" # å¯¹è¯å†å²
        }

        # åŠ è½½è®°å¿†
        self.memory = self._load_all()

        # LangMem å¢å¼ºç»„ä»¶
        self.scorer = ImportanceScorer()
        self.retriever = SemanticRetriever(self.scorer)
        self.cleaner = MemoryCleaner(self.scorer)

    def _load_all(self) -> Dict[str, Any]:
        """åŠ è½½æ‰€æœ‰è®°å¿†"""
        memory = {}
        for key, path in self.files.items():
            if path.exists():
                try:
                    with open(path, 'r', encoding='utf-8') as f:
                        memory[key] = json.load(f)
                except Exception as e:
                    print(f"âš ï¸ åŠ è½½ {key} å¤±è´¥: {e}")
                    memory[key] = self._get_default_structure(key)
            else:
                memory[key] = self._get_default_structure(key)
        return memory

    def _get_default_structure(self, memory_type: str) -> Any:
        """è·å–é»˜è®¤ç»“æ„"""
        defaults = {
            'contexts': {
                'total_contexts': 0,
                'contexts_by_topic': defaultdict(int),
                'contexts': []
            },
            'decisions': {
                'total_decisions': 0,
                'tool_usage_stats': defaultdict(int),
                'decisions': []
            },
            'preferences': {
                'coding_style': {},
                'preferred_tools': {},
                'communication_style': {},
                'frequently_used_commands': {}
            },
            'projects': {
                'active_projects': [],
                'project_knowledge': {},
                'project_patterns': {}
            },
            'evolution': {
                'version_history': [],
                'capability_changes': [],
                'milestones': []
            },
            'conversations': {
                'total_conversations': 0,
                'conversations': []
            }
        }
        return defaults.get(memory_type, {})

    def save(self, memory_type: str = None):
        """ä¿å­˜è®°å¿†"""
        if memory_type:
            self._save_one(memory_type)
        else:
            for key in self.files.keys():
                self._save_one(key)

    def _save_one(self, memory_type: str):
        """ä¿å­˜å•ä¸ªè®°å¿†ç±»å‹"""
        if memory_type not in self.files:
            return

        path = self.files[memory_type]
        try:
            with open(path, 'w', encoding='utf-8') as f:
                json.dump(self.memory[memory_type], f, ensure_ascii=False, indent=2)
        except Exception as e:
            print(f"âš ï¸ ä¿å­˜ {memory_type} å¤±è´¥: {e}")

    def add_context(self, context: Dict[str, Any]):
        """æ·»åŠ ä¸Šä¸‹æ–‡è®°å¿†"""
        ctx = {
            'timestamp': datetime.now().isoformat(),
            'session_id': context.get('session_id', ''),
            'topic': context.get('topic', ''),
            'summary': context.get('summary', ''),
            'key_points': context.get('key_points', []),
            'tools_used': context.get('tools_used', []),
            'decisions_made': context.get('decisions_made', []),
            'outcomes': context.get('outcomes', ''),
            'priority': context.get('priority', 'normal'),  # æ–°å¢ï¼šä¼˜å…ˆçº§
            'tags': context.get('tags', [])  # æ–°å¢ï¼šæ ‡ç­¾
        }

        self.memory['contexts']['contexts'].append(ctx)
        self.memory['contexts']['total_contexts'] += 1

        # ç»Ÿè®¡ä¸»é¢˜
        topic = context.get('topic', 'unknown')
        if topic not in self.memory['contexts']['contexts_by_topic']:
            self.memory['contexts']['contexts_by_topic'][topic] = 0
        self.memory['contexts']['contexts_by_topic'][topic] += 1

        self.save('contexts')

    def add_decision(self, decision: Dict[str, Any]):
        """æ·»åŠ å†³ç­–è®°å¿†"""
        dec = {
            'timestamp': datetime.now().isoformat(),
            'task_type': decision.get('task_type', ''),
            'tool_chosen': decision.get('tool_chosen', ''),
            'alternatives': decision.get('alternatives', []),
            'reasoning': decision.get('reasoning', ''),
            'success': decision.get('success', True),
            'lesson_learned': decision.get('lesson_learned', '')
        }

        self.memory['decisions']['decisions'].append(dec)
        self.memory['decisions']['total_decisions'] += 1

        # ç»Ÿè®¡å·¥å…·ä½¿ç”¨
        tool = decision.get('tool_chosen', '')
        if tool:
            if tool not in self.memory['decisions']['tool_usage_stats']:
                self.memory['decisions']['tool_usage_stats'][tool] = 0
            self.memory['decisions']['tool_usage_stats'][tool] += 1

        self.save('decisions')

    def update_preferences(self, preferences: Dict[str, Any]):
        """æ›´æ–°ç”¨æˆ·åå¥½"""
        for key, value in preferences.items():
            if key in self.memory['preferences']:
                if isinstance(value, dict):
                    self.memory['preferences'][key].update(value)
                else:
                    self.memory['preferences'][key] = value

        self.save('preferences')

    def add_conversation(self, conversation: Dict[str, Any]):
        """æ·»åŠ å¯¹è¯è®°å½•"""
        conv = {
            'timestamp': datetime.now().isoformat(),
            'session_id': conversation.get('session_id', ''),
            'user_query': conversation.get('user_query', ''),
            'my_response': conversation.get('my_response', ''),
            'tools_used': conversation.get('tools_used', []),
            'outcome': conversation.get('outcome', ''),
            'user_satisfaction': conversation.get('user_satisfaction', None),
            'follow_up_actions': conversation.get('follow_up_actions', [])
        }

        self.memory['conversations']['conversations'].append(conv)
        self.memory['conversations']['total_conversations'] += 1

        self.save('conversations')

    def get_relevant_contexts(self, topic: str, limit: int = 5) -> List[Dict]:
        """è·å–ç›¸å…³çš„ä¸Šä¸‹æ–‡"""
        contexts = self.memory['contexts']['contexts']

        # ç®€å•çš„å…³é”®è¯åŒ¹é…ï¼ˆå®é™…åº”è¯¥ç”¨è¯­ä¹‰æœç´¢ï¼‰
        relevant = []
        for ctx in contexts:
            if topic.lower() in ctx.get('topic', '').lower() or \
               topic.lower() in ctx.get('summary', '').lower():
                relevant.append(ctx)
                if len(relevant) >= limit:
                    break

        return relevant

    def get_tool_preferences(self, task_type: str) -> Optional[str]:
        """è·å–å·¥å…·åå¥½"""
        # ä»å†³ç­–å†å²ä¸­å­¦ä¹ 
        decisions = self.memory['decisions']['decisions']

        # ç»Ÿè®¡è¯¥ä»»åŠ¡ç±»å‹ä¸‹æœ€å¸¸ç”¨çš„å·¥å…·
        tool_counts = defaultdict(int)
        for dec in decisions:
            if dec.get('task_type') == task_type and dec.get('success'):
                tool = dec.get('tool_chosen', '')
                tool_counts[tool] += 1

        if tool_counts:
            return max(tool_counts.items(), key=lambda x: x[1])[0]
        return None

    def get_high_priority_contexts(self, limit: int = 10) -> List[Dict]:
        """è·å–é«˜ä¼˜å…ˆçº§è®°å¿†"""
        contexts = self.memory['contexts']['contexts']
        high_priority = [ctx for ctx in contexts if ctx.get('priority') == 'high']
        return high_priority[:limit]

    def get_contexts_by_tag(self, tag: str, limit: int = 10) -> List[Dict]:
        """æŒ‰æ ‡ç­¾è·å–è®°å¿†"""
        contexts = self.memory['contexts']['contexts']
        tagged = [ctx for ctx in contexts if tag in ctx.get('tags', [])]
        return tagged[:limit]

    def get_recent_contexts(self, limit: int = 10) -> List[Dict]:
        """è·å–æœ€è¿‘çš„è®°å¿†"""
        contexts = self.memory['contexts']['contexts']
        return contexts[-limit:]

    def search_all_contexts(self, keyword: str, limit: int = 20) -> List[Dict]:
        """å…¨å±€æœç´¢è®°å¿†"""
        contexts = self.memory['contexts']['contexts']
        results = []
        keyword_lower = keyword.lower()

        for ctx in contexts:
            # åœ¨å¤šä¸ªå­—æ®µä¸­æœç´¢
            searchable_text = ' '.join([
                ctx.get('topic', ''),
                ctx.get('summary', ''),
                ' '.join(ctx.get('key_points', [])),
                ' '.join(ctx.get('decisions_made', [])),
                ctx.get('outcomes', '')
            ]).lower()

            if keyword_lower in searchable_text:
                results.append(ctx)
                if len(results) >= limit:
                    break

        return results

    def get_statistics(self) -> Dict[str, Any]:
        """è·å–è®°å¿†ç»Ÿè®¡"""
        return {
            'total_contexts': self.memory['contexts']['total_contexts'],
            'total_decisions': self.memory['decisions']['total_decisions'],
            'total_conversations': self.memory['conversations']['total_conversations'],
            'topics_covered': list(self.memory['contexts']['contexts_by_topic'].keys()),
            'most_used_tools': dict(self.memory['decisions']['tool_usage_stats']),
            'memory_size_kb': sum(f.stat().st_size for f in self.files.values() if f.exists()) / 1024
        }

    # ========================================================================
    # LangMem å¢å¼ºæ–¹æ³•
    # ========================================================================

    def calculate_importance(self, memory: Dict[str, Any]) -> float:
        """è®¡ç®—å•æ¡è®°å¿†çš„é‡è¦æ€§åˆ†æ•°"""
        return self.scorer.calculate(memory)

    def semantic_search(self, query: str, top_k: int = 5,
                        min_score: float = 20.0) -> List[Dict[str, Any]]:
        """
        è¯­ä¹‰æœç´¢è®°å¿†

        å‚æ•°:
            query: æœç´¢æŸ¥è¯¢
            top_k: è¿”å›å‰Kä¸ªç»“æœ
            min_score: æœ€ä½ç›¸å…³æ€§åˆ†æ•°

        è¿”å›:
            ç›¸å…³è®°å¿†åˆ—è¡¨ï¼ˆåŒ…å« _relevance_score å­—æ®µï¼‰
        """
        contexts = self.memory['contexts']['contexts']
        return self.retriever.search(contexts, query, top_k, min_score)

    def get_top_memories(self, limit: int = 10) -> List[Dict[str, Any]]:
        """è·å–æœ€é‡è¦çš„è®°å¿†ï¼ˆæŒ‰é‡è¦æ€§åˆ†æ•°æ’åºï¼‰"""
        contexts = self.memory['contexts']['contexts']

        # è®¡ç®—æ‰€æœ‰è®°å¿†çš„é‡è¦æ€§åˆ†æ•°
        scored_memories = []
        for ctx in contexts:
            score = self.scorer.calculate(ctx)
            scored_memories.append((ctx, score))

        # æŒ‰åˆ†æ•°æ’åº
        scored_memories.sort(key=lambda x: x[1], reverse=True)

        # è¿”å›top_kï¼ˆé™„å¸¦åˆ†æ•°ï¼‰
        results = []
        for memory, score in scored_memories[:limit]:
            memory_with_score = memory.copy()
            memory_with_score['_importance_score'] = round(score, 2)
            results.append(memory_with_score)

        return results

    def analyze_memory_health(self) -> Dict[str, Any]:
        """åˆ†æè®°å¿†åº“å¥åº·çŠ¶å†µ"""
        contexts = self.memory['contexts']['contexts']
        return self.cleaner.suggest_cleanup(contexts)

    def cleanup_memories(self, threshold: float = 30.0,
                         keep_recent_days: int = 30,
                         dry_run: bool = True) -> Dict[str, Any]:
        """
        æ¸…ç†ä½åˆ†è®°å¿†

        å‚æ•°:
            threshold: é‡è¦æ€§åˆ†æ•°é˜ˆå€¼ï¼ˆä½äºæ­¤åˆ†æ•°å°†è¢«æ¸…ç†ï¼‰
            keep_recent_days: ä¿ç•™æœ€è¿‘Nå¤©çš„è®°å¿†
            dry_run: æ¨¡æ‹Ÿè¿è¡Œï¼ˆä¸å®é™…åˆ é™¤ï¼‰

        è¿”å›:
            æ¸…ç†ç»Ÿè®¡ä¿¡æ¯
        """
        contexts = self.memory['contexts']['contexts']
        result = self.cleaner.cleanup_low_score(
            contexts, threshold, keep_recent_days, dry_run
        )

        # å¦‚æœä¸æ˜¯æ¨¡æ‹Ÿè¿è¡Œï¼Œå®é™…æ›´æ–°è®°å¿†
        if not dry_run:
            self.memory['contexts']['contexts'] = result['cleaned_memories']
            self.memory['contexts']['total_contexts'] = len(result['cleaned_memories'])
            self.save('contexts')

        return result

    def get_importance_distribution(self) -> Dict[str, Any]:
        """è·å–é‡è¦æ€§åˆ†æ•°åˆ†å¸ƒç»Ÿè®¡"""
        contexts = self.memory['contexts']['contexts']
        scores = [self.scorer.calculate(ctx) for ctx in contexts]

        if not scores:
            return {'error': 'æ— è®°å¿†æ•°æ®'}

        # åˆ†çº§ç»Ÿè®¡
        high = sum(1 for s in scores if s >= 70)
        medium = sum(1 for s in scores if 50 <= s < 70)
        low = sum(1 for s in scores if 30 <= s < 50)
        very_low = sum(1 for s in scores if s < 30)

        return {
            'total': len(scores),
            'average': round(sum(scores) / len(scores), 2),
            'min': round(min(scores), 2),
            'max': round(max(scores), 2),
            'distribution': {
                'high (70-100)': high,
                'medium (50-69)': medium,
                'low (30-49)': low,
                'very_low (0-29)': very_low
            },
            'percentiles': {
                'p50': round(sorted(scores)[len(scores)//2], 2),
                'p75': round(sorted(scores)[int(len(scores)*0.75)], 2) if len(scores) > 1 else 0,
                'p90': round(sorted(scores)[int(len(scores)*0.9)], 2) if len(scores) > 1 else 0
            }
        }


# ============================================================================
# è®°å¿†ç®¡ç†å™¨
# ============================================================================

class ClaudeMemory:
    """Claude Code è®°å¿†ç®¡ç†å™¨ (v2.0 - æ”¯æŒå‘é‡è¯­ä¹‰æœç´¢)"""

    def __init__(self, workspace_root: Optional[Path] = None, enable_semantic: bool = True):
        if workspace_root is None:
            # è‡ªåŠ¨æ£€æµ‹å·¥ä½œåŒºæ ¹ç›®å½•
            workspace_root = Path(__file__).parent.parent

        self.store = MemoryStore(workspace_root)
        self.current_session = self._generate_session_id()

        # v2.0æ–°å¢ï¼šè¯­ä¹‰è®°å¿†ï¼ˆå¯é€‰å¯ç”¨ï¼‰
        self.enable_semantic = enable_semantic
        self.semantic_memory = None

        if enable_semantic:
            try:
                from semantic_memory import SemanticMemory
                self.semantic_memory = SemanticMemory(workspace_root)
                print("âœ… è¯­ä¹‰è®°å¿†å·²å¯ç”¨")
            except ImportError as e:
                print(f"âš ï¸ è¯­ä¹‰è®°å¿†æœªå¯ç”¨: {e}")
                print("ğŸ’¡ æç¤º: è¿è¡Œ pip install chromadb sentence-transformers")
                self.enable_semantic = False

    def _generate_session_id(self) -> str:
        """ç”Ÿæˆä¼šè¯ID"""
        return f"session_{datetime.now().strftime('%Y%m%d_%H%M%S')}"

    def remember_context(self, topic: str, summary: str, key_points: List[str],
                        tools_used: List[str], decisions_made: List[str],
                        outcomes: str, priority: str = 'normal', tags: List[str] = None):
        """è®°ä½å¯¹è¯ä¸Šä¸‹æ–‡ (v2.0 - åŒæ—¶ä¿å­˜åˆ°å‘é‡æ•°æ®åº“)"""
        context = {
            'session_id': self.current_session,
            'topic': topic,
            'summary': summary,
            'key_points': key_points,
            'tools_used': tools_used,
            'decisions_made': decisions_made,
            'outcomes': outcomes,
            'priority': priority,
            'tags': tags or []
        }

        # ä¿å­˜åˆ°JSONå­˜å‚¨
        self.store.add_context(context)

        # v2.0æ–°å¢ï¼šåŒæ—¶ä¿å­˜åˆ°å‘é‡æ•°æ®åº“
        if self.enable_semantic and self.semantic_memory:
            memory_id = f"ctx_{context['session_id']}_{datetime.now().timestamp()}"
            text = f"{topic}. {summary}"

            # å‡†å¤‡å…ƒæ•°æ®
            metadata = {
                'topic': topic,
                'session_id': self.current_session,
                'priority': priority,
                'tags': ','.join(tags or []),
                'type': 'context'
            }

            self.semantic_memory.add_memory(memory_id, text, metadata)

    def remember_decision(self, task_type: str, tool_chosen: str,
                         alternatives: List[str], reasoning: str,
                         success: bool, lesson_learned: str = ""):
        """è®°ä½å†³ç­–"""
        decision = {
            'task_type': task_type,
            'tool_chosen': tool_chosen,
            'alternatives': alternatives,
            'reasoning': reasoning,
            'success': success,
            'lesson_learned': lesson_learned
        }
        self.store.add_decision(decision)

    def remember_conversation(self, user_query: str, my_response: str,
                            tools_used: List[str], outcome: str):
        """è®°ä½å¯¹è¯"""
        conversation = {
            'session_id': self.current_session,
            'user_query': user_query,
            'my_response': my_response,
            'tools_used': tools_used,
            'outcome': outcome
        }
        self.store.add_conversation(conversation)

    def recall(self, topic: str) -> List[Dict]:
        """å›å¿†ç›¸å…³ä¸Šä¸‹æ–‡"""
        return self.store.get_relevant_contexts(topic)

    def recall_high_priority(self, limit: int = 10) -> List[Dict]:
        """å›å¿†é«˜ä¼˜å…ˆçº§ä¸Šä¸‹æ–‡"""
        return self.store.get_high_priority_contexts(limit)

    def recall_by_tag(self, tag: str, limit: int = 10) -> List[Dict]:
        """æŒ‰æ ‡ç­¾å›å¿†ä¸Šä¸‹æ–‡"""
        return self.store.get_contexts_by_tag(tag, limit)

    def recall_recent(self, limit: int = 10) -> List[Dict]:
        """å›å¿†æœ€è¿‘çš„ä¸Šä¸‹æ–‡"""
        return self.store.get_recent_contexts(limit)

    def search_memory(self, keyword: str, limit: int = 20) -> List[Dict]:
        """å…¨å±€æœç´¢è®°å¿†"""
        return self.store.search_all_contexts(keyword, limit)

    def suggest_tool(self, task_type: str) -> Optional[str]:
        """åŸºäºå†å²å»ºè®®å·¥å…·"""
        return self.store.get_tool_preferences(task_type)

    def learn_preferences(self, preferences: Dict[str, Any]):
        """å­¦ä¹ ç”¨æˆ·åå¥½"""
        self.store.update_preferences(preferences)

    def get_memory_stats(self) -> Dict[str, Any]:
        """è·å–è®°å¿†ç»Ÿè®¡"""
        return self.store.get_statistics()

    def calculate_importance(self, memory: Dict[str, Any]) -> float:
        """è®¡ç®—å•æ¡è®°å¿†çš„é‡è¦æ€§åˆ†æ•°"""
        return self.store.calculate_importance(memory)

    # ========================================================================
    # LangMem å¢å¼ºæ–¹æ³•ï¼ˆä»£ç†åˆ° MemoryStoreï¼‰
    # ========================================================================

    def semantic_search(self, query: str, top_k: int = 5,
                        min_score: float = 20.0) -> List[Dict[str, Any]]:
        """è¯­ä¹‰æœç´¢è®°å¿† (v2.0 - å‘é‡è¯­ä¹‰æœç´¢)"""
        # v2.0æ–°å¢ï¼šä¼˜å…ˆä½¿ç”¨å‘é‡è¯­ä¹‰æœç´¢
        if self.enable_semantic and self.semantic_memory:
            return self.semantic_memory.search(query, top_k=top_k)
        else:
            # å›é€€åˆ°åŸæœ‰çš„å…³é”®è¯è¯­ä¹‰æœç´¢
            return self.store.semantic_search(query, top_k, min_score)

    def hybrid_search(self, query: str, top_k: int = 5,
                     semantic_weight: float = 0.7) -> List[Dict[str, Any]]:
        """
        æ··åˆæœç´¢ (v2.0æ–°å¢)

        ç»“åˆå‘é‡è¯­ä¹‰æœç´¢å’Œå…³é”®è¯æœç´¢ï¼Œæä¾›æœ€ä½³ç»“æœ

        å‚æ•°:
            query: æœç´¢æŸ¥è¯¢
            top_k: è¿”å›å‰Kä¸ªç»“æœ
            semantic_weight: è¯­ä¹‰æœç´¢æƒé‡ (0-1ï¼Œé»˜è®¤0.7)
        """
        if not (self.enable_semantic and self.semantic_memory):
            # è¯­ä¹‰æœç´¢æœªå¯ç”¨ï¼Œä½¿ç”¨å…³é”®è¯æœç´¢
            return self.store.semantic_search(query, top_k)

        # è·å–å…³é”®è¯æœç´¢ç»“æœ
        keyword_results = self.store.semantic_search(query, top_k * 2)

        # è½¬æ¢ä¸ºç»Ÿä¸€æ ¼å¼
        formatted_keyword_results = []
        for kw_result in keyword_results:
            formatted_keyword_results.append({
                'id': kw_result.get('timestamp', ''),
                'text': f"{kw_result.get('topic', '')}. {kw_result.get('summary', '')}",
                'metadata': kw_result,
                '_relevance_score': kw_result.get('_relevance_score', 0)
            })

        # æ‰§è¡Œæ··åˆæœç´¢
        hybrid_results = self.semantic_memory.hybrid_search(
            query=query,
            keyword_results=formatted_keyword_results,
            top_k=top_k,
            semantic_weight=semantic_weight
        )

        return hybrid_results

    def get_top_memories(self, limit: int = 10) -> List[Dict[str, Any]]:
        """è·å–æœ€é‡è¦çš„è®°å¿†"""
        return self.store.get_top_memories(limit)

    def analyze_memory_health(self) -> Dict[str, Any]:
        """åˆ†æè®°å¿†åº“å¥åº·çŠ¶å†µ"""
        return self.store.analyze_memory_health()

    def cleanup_memories(self, threshold: float = 30.0,
                         keep_recent_days: int = 30,
                         dry_run: bool = True) -> Dict[str, Any]:
        """æ¸…ç†ä½åˆ†è®°å¿†"""
        return self.store.cleanup_memories(threshold, keep_recent_days, dry_run)

    def get_importance_distribution(self) -> Dict[str, Any]:
        """è·å–é‡è¦æ€§åˆ†æ•°åˆ†å¸ƒ"""
        return self.store.get_importance_distribution()

    def print_memory_summary(self):
        """æ‰“å°è®°å¿†æ‘˜è¦"""
        stats = self.get_memory_stats()

        print("\n" + "=" * 70)
        print("ğŸ§  Claude Code è®°å¿†ç³»ç»Ÿ")
        print("=" * 70)

        print(f"\nğŸ“Š ç»Ÿè®¡ä¿¡æ¯:")
        print(f"   - å¯¹è¯ä¸Šä¸‹æ–‡: {stats['total_contexts']} æ¡")
        print(f"   - å†³ç­–è®°å½•: {stats['total_decisions']} æ¡")
        print(f"   - å¯¹è¯å†å²: {stats['total_conversations']} æ¡")
        print(f"   - è®°å¿†å ç”¨: {stats['memory_size_kb']:.1f} KB")

        if stats['topics_covered']:
            print(f"\nğŸ“š æ¶µç›–ä¸»é¢˜:")
            for topic in list(stats['topics_covered'])[:10]:
                count = self.store.memory['contexts']['contexts_by_topic'][topic]
                print(f"   - {topic} ({count} æ¬¡)")

        if stats['most_used_tools']:
            print(f"\nğŸ› ï¸ å¸¸ç”¨å·¥å…·:")
            for tool, count in sorted(stats['most_used_tools'].items(),
                                     key=lambda x: x[1], reverse=True)[:5]:
                print(f"   - {tool} ({count} æ¬¡)")

        print("\n" + "=" * 70)


# ============================================================================
# è®°å¿†å¢å¼ºAgent
# ============================================================================

class MemoryEnhancedAgent:
    """
    è®°å¿†å¢å¼ºçš„AgentåŸºç±»

    ä»»ä½•ç»§æ‰¿æ­¤ç±»çš„Agentéƒ½å°†è·å¾—æŒä¹…åŒ–è®°å¿†èƒ½åŠ›
    """

    def __init__(self, workspace_root: Optional[Path] = None):
        self.memory = ClaudeMemory(workspace_root)

    def recall_before_action(self, topic: str) -> List[Dict]:
        """åœ¨è¡ŒåŠ¨å‰å›å¿†ç›¸å…³ç»éªŒ"""
        return self.memory.recall(topic)

    def learn_from_action(self, task_type: str, tool_used: str,
                         reasoning: str, success: bool):
        """ä»è¡ŒåŠ¨ä¸­å­¦ä¹ """
        self.memory.remember_decision(
            task_type=task_type,
            tool_chosen=tool_used,
            alternatives=[],
            reasoning=reasoning,
            success=success,
            lesson_learned=f"ä½¿ç”¨{tool_used}{'æˆåŠŸ' if success else 'å¤±è´¥'}"
        )

    def suggest_based_on_experience(self, task_type: str) -> Optional[str]:
        """åŸºäºç»éªŒå»ºè®®å·¥å…·"""
        return self.memory.suggest_tool(task_type)

    # ä»£ç†ClaudeMemoryçš„å…¶ä»–æ–¹æ³•
    def remember_context(self, topic: str, summary: str, key_points: List[str],
                        tools_used: List[str], decisions_made: List[str],
                        outcomes: str):
        """è®°ä½å¯¹è¯ä¸Šä¸‹æ–‡"""
        self.memory.remember_context(topic, summary, key_points, tools_used, decisions_made, outcomes)

    def remember_conversation(self, user_query: str, my_response: str,
                            tools_used: List[str], outcome: str):
        """è®°ä½å¯¹è¯"""
        self.memory.remember_conversation(user_query, my_response, tools_used, outcome)

    def learn_preferences(self, preferences: Dict[str, Any]):
        """å­¦ä¹ ç”¨æˆ·åå¥½"""
        self.memory.learn_preferences(preferences)

    def get_memory_stats(self) -> Dict[str, Any]:
        """è·å–è®°å¿†ç»Ÿè®¡"""
        return self.memory.get_memory_stats()

    # ä»£ç†ClaudeMemoryçš„recallæ–¹æ³•
    def recall(self, topic: str) -> List[Dict]:
        """å›å¿†ç›¸å…³ä¸Šä¸‹æ–‡"""
        return self.memory.recall(topic)


# ============================================================================
# æ¼”ç¤ºç¨‹åº
# ============================================================================

def demo_memory_system():
    """æ¼”ç¤ºè®°å¿†ç³»ç»Ÿ"""

    print("""
â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—
â•‘                                                                    â•‘
â•‘              Claude Code è®°å¿†æŒä¹…åŒ–ç³»ç»Ÿæ¼”ç¤º                         â•‘
â•‘                                                                    â•‘
â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    """)

    # åˆ›å»ºè®°å¿†ç³»ç»Ÿ
    memory = ClaudeMemory()

    print("\nğŸ“ æ¨¡æ‹Ÿå­¦ä¹ è¿‡ç¨‹...")
    print("-" * 70)

    # æ¨¡æ‹Ÿ1: å­¦ä¹ æ–‡ä»¶è¯»å–åå¥½
    print("\n1ï¸âƒ£ å­¦ä¹ : æ–‡ä»¶è¯»å–ä»»åŠ¡")
    memory.remember_decision(
        task_type="read_file",
        tool_chosen="Read",
        alternatives=["Bash: cat", "Grep"],
        reasoning="Readå·¥å…·æ˜¯ä¸“é—¨ä¸ºæ–‡ä»¶è¯»å–è®¾è®¡çš„ï¼Œæ›´å¿«æ›´å‡†ç¡®",
        success=True,
        lesson_learned="ä¼˜å…ˆä½¿ç”¨Readå·¥å…·è¯»å–æ–‡ä»¶"
    )

    # æ¨¡æ‹Ÿ2: å­¦ä¹ ä»£ç æœç´¢åå¥½
    print("2ï¸âƒ£ å­¦ä¹ : ä»£ç æœç´¢ä»»åŠ¡")
    memory.remember_decision(
        task_type="search_code",
        tool_chosen="Grep",
        alternatives=["Glob", "Task: Explore"],
        reasoning="Grepæ”¯æŒæ­£åˆ™è¡¨è¾¾å¼ï¼Œé€‚åˆç²¾ç¡®æœç´¢",
        success=True
    )

    # æ¨¡æ‹Ÿ3: è®°ä½å¯¹è¯ä¸Šä¸‹æ–‡
    print("3ï¸âƒ£ å­¦ä¹ : å¤šAgentç³»ç»Ÿå¯¹è¯")
    memory.remember_context(
        topic="å¤šAgentç³»ç»Ÿå¼€å‘",
        summary="åˆ›å»ºäº†åŸºäºWorkflowEngineçš„å¤šAgentæ¼”ç¤ºç³»ç»Ÿ",
        key_points=[
            "å®ç°äº†4ä¸ªä¸“é—¨Agentï¼šCoordinator, Analyst, Processor, Reviewer",
            "ä½¿ç”¨WorkflowGraphè¿›è¡Œå·¥ä½œæµç¼–æ’",
            "çŠ¶æ€åœ¨Agentä¹‹é—´ä¼ é€’"
        ],
        tools_used=["Write", "Bash", "Read"],
        decisions_made=["ä½¿ç”¨workflow_engineè€ŒéLangGraph", "é€‰æ‹©ä¸²è¡Œåä½œæ¨¡å¼"],
        outcomes="æˆåŠŸè¿è¡Œæ¼”ç¤ºï¼Œå±•ç¤ºäº†Agentåä½œèƒ½åŠ›"
    )

    # æ¨¡æ‹Ÿ4: è®°ä½ç”¨æˆ·åå¥½
    print("4ï¸âƒ£ å­¦ä¹ : ç”¨æˆ·åå¥½")
    memory.learn_preferences({
        'coding_style': {
            'language': 'Python',
            'naming_convention': 'snake_case'
        },
        'preferred_tools': {
            'file_operations': 'Read/Edit/Writeä¸“ç”¨å·¥å…·'
        }
    })

    # æ¨¡æ‹Ÿ5: è®°ä½å¯¹è¯
    print("5ï¸âƒ£ å­¦ä¹ : å¯¹è¯å†å²")
    memory.remember_conversation(
        user_query="æ¼”ç¤ºä¸€ä¸ªç®€å•çš„å¤šAgentç³»ç»ŸåŸå‹ï¼Ÿ",
        my_response="åˆ›å»ºäº†multi_agent_demo.pyï¼ŒåŒ…å«4ä¸ªAgent...",
        tools_used=["Write", "Bash", "Read"],
        outcome="æˆåŠŸæ¼”ç¤ºäº†å¤šAgentåä½œ"
    )

    # æ˜¾ç¤ºè®°å¿†æ‘˜è¦
    print("\n" + "=" * 70)
    print("ğŸ“Š è®°å¿†ç³»ç»Ÿå­¦ä¹ å®Œæˆ")
    print("=" * 70)
    memory.print_memory_summary()

    # æ¼”ç¤ºå›å¿†
    print("\nğŸ”® æ¼”ç¤ºè®°å¿†å›å¿†...")
    print("-" * 70)

    print("\nğŸ’­ å›å¿†: å…³äº'å¤šAgent'çš„ä¸Šä¸‹æ–‡")
    contexts = memory.recall("å¤šAgent")
    for ctx in contexts:
        print(f"\n   æ—¶é—´: {ctx['timestamp']}")
        print(f"   ä¸»é¢˜: {ctx['topic']}")
        print(f"   æ‘˜è¦: {ctx['summary']}")

    print("\nğŸ› ï¸ å»ºè®®: åŸºäºç»éªŒï¼Œæ–‡ä»¶è¯»å–åº”è¯¥ç”¨")
    suggested = memory.suggest_tool("read_file")
    print(f"   â†’ {suggested or 'æ— å†å²æ•°æ®'}")

    print("\n" + "=" * 70)
    print("âœ… æ¼”ç¤ºå®Œæˆï¼")
    print("\nğŸ’¡ è¯´æ˜:")
    print("   - æ‰€æœ‰è®°å¿†å·²ä¿å­˜åˆ°: 06_Learning_Journal/claude_memory/")
    print("   - ä¸‹æ¬¡ä¼šè¯å¯ä»¥ç»§ç»­ä½¿ç”¨è¿™äº›è®°å¿†")
    print("   - è®°å¿†ä¼šæŒç»­ç´¯ç§¯å’Œè¿›åŒ–")
    print("=" * 70)


if __name__ == "__main__":
    demo_memory_system()
