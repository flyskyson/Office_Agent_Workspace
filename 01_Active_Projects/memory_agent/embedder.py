#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
æ–‡æœ¬åµŒå…¥æ¨¡å—
ä½¿ç”¨sentence-transformersç”Ÿæˆæœ¬åœ°å‘é‡åµŒå…¥
"""

import yaml
from pathlib import Path
from sentence_transformers import SentenceTransformer
from typing import List, Union


class TextEmbedder:
    """æ–‡æœ¬åµŒå…¥å™¨ - ç”Ÿæˆæœ¬åœ°å‘é‡"""

    def __init__(self, config_path="config.yaml"):
        """åˆå§‹åŒ–åµŒå…¥å™¨"""
        # åŠ è½½é…ç½®
        config_path = Path(__file__).parent / config_path
        with open(config_path, 'r', encoding='utf-8') as f:
            config = yaml.safe_load(f)

        embedding_config = config['embedding']

        print(f"ğŸ“¦ åŠ è½½åµŒå…¥æ¨¡å‹: {embedding_config['model_name']}")
        print(f"   è®¾å¤‡: {embedding_config['device']}")

        # åŠ è½½æ¨¡å‹ï¼ˆé¦–æ¬¡ä¼šä¸‹è½½ï¼Œçº¦500MBï¼‰
        self.model = SentenceTransformer(
            embedding_config['model_name'],
            device=embedding_config['device']
        )

        self.batch_size = embedding_config.get('batch_size', 32)

        # è·å–å‘é‡ç»´åº¦
        self.embedding_dim = self.model.get_sentence_embedding_dimension()
        print(f"âœ… æ¨¡å‹åŠ è½½å®Œæˆï¼å‘é‡ç»´åº¦: {self.embedding_dim}")

    def embed_texts(self, texts: List[str], show_progress=False) -> List[List[float]]:
        """
        åµŒå…¥æ–‡æœ¬åˆ—è¡¨

        Args:
            texts: æ–‡æœ¬åˆ—è¡¨
            show_progress: æ˜¯å¦æ˜¾ç¤ºè¿›åº¦æ¡

        Returns:
            å‘é‡åˆ—è¡¨
        """
        if not texts:
            return []

        embeddings = self.model.encode(
            texts,
            batch_size=self.batch_size,
            show_progress_bar=show_progress,
            convert_to_numpy=True
        )

        return embeddings.tolist()

    def embed_text(self, text: str) -> List[float]:
        """
        åµŒå…¥å•ä¸ªæ–‡æœ¬

        Args:
            text: æ–‡æœ¬å†…å®¹

        Returns:
            å‘é‡
        """
        embedding = self.model.encode(
            text,
            convert_to_numpy=True
        )
        return embedding.tolist()

    @property
    def dimension(self) -> int:
        """è¿”å›å‘é‡ç»´åº¦"""
        return self.embedding_dim


def main():
    """æµ‹è¯•åµŒå…¥å™¨"""
    print("=" * 70)
    print("æ–‡æœ¬åµŒå…¥å™¨æµ‹è¯•")
    print("=" * 70)

    embedder = TextEmbedder()

    # æµ‹è¯•æ–‡æœ¬
    test_texts = [
        "Pythonæ–‡ä»¶æ‰¹é‡é‡å‘½åæŠ€å·§",
        "ä½¿ç”¨Path.rename()æ–¹æ³•é‡å‘½åæ–‡ä»¶",
        "æœºå™¨å­¦ä¹ æ¨¡å‹è®­ç»ƒ",
        "ä»Šå¤©å¤©æ°”å¾ˆå¥½"
    ]

    print("\nğŸ“ æµ‹è¯•æ–‡æœ¬:")
    for i, text in enumerate(test_texts, 1):
        print(f"  {i}. {text}")

    print("\nğŸ”„ ç”ŸæˆåµŒå…¥å‘é‡...")
    embeddings = embedder.embed_texts(test_texts, show_progress=True)

    print(f"\nâœ… ç”Ÿæˆäº† {len(embeddings)} ä¸ªå‘é‡")
    print(f"   å‘é‡ç»´åº¦: {len(embeddings[0])}")

    # è®¡ç®—ç›¸ä¼¼åº¦
    import numpy as np

    def cosine_similarity(a, b):
        return np.dot(a, b) / (np.linalg.norm(a) * np.linalg.norm(b))

    print("\nğŸ“Š ç›¸ä¼¼åº¦çŸ©é˜µ:")
    print("   " + "  ".join([f"{i+1}" for i in range(len(test_texts))]))

    for i in range(len(test_texts)):
        row = []
        for j in range(len(test_texts)):
            if i == j:
                row.append("1.00")
            else:
                sim = cosine_similarity(embeddings[i], embeddings[j])
                row.append(f"{sim:.2f}")
        print(f" {i+1} " + "  ".join(row))


if __name__ == "__main__":
    main()
