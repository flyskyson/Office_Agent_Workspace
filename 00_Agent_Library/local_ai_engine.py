#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
æœ¬åœ° AI å¼•æ“ - ä¼˜åŒ–ç‰ˆ
æ”¯æŒå¤šæ¨¡å‹ã€GPU åŠ é€Ÿã€æ™ºèƒ½é™çº§
"""

import os
import yaml
import logging
from pathlib import Path
from typing import Dict, List, Optional, Any, Tuple
from dataclasses import dataclass
from enum import Enum
import time


class AIEngine(Enum):
    """AI å¼•æ“ç±»å‹"""
    PADDLEOCR = "paddleocr"
    BAIDU_OCR = "baidu_ocr"
    SENTENCE_TRANSFORMER = "sentence_transformer"
    DEEPSEEK = "deepseek"
    OLLAMA = "ollama"


@dataclass
class OCRResult:
    """OCR ç»“æœ"""
    text: str
    confidence: float
    engine: str
    processing_time: float
    raw_data: Optional[Dict] = None


class LocalAIEngine:
    """æœ¬åœ° AI å¼•æ“ - ç»Ÿä¸€æ¥å£"""

    def __init__(self, config_path: Optional[str] = None):
        """
        åˆå§‹åŒ–æœ¬åœ° AI å¼•æ“

        Args:
            config_path: é…ç½®æ–‡ä»¶è·¯å¾„
        """
        self.config = self._load_config(config_path)
        self.logger = self._setup_logging()
        self._engines = {}
        self._cache = {}

        # åˆå§‹åŒ–å¼•æ“
        self._initialize_engines()

    def _load_config(self, config_path: Optional[str]) -> Dict:
        """åŠ è½½é…ç½®æ–‡ä»¶"""
        if config_path is None:
            # é»˜è®¤é…ç½®è·¯å¾„
            default_path = Path(__file__).parent.parent / \
                "01_Active_Projects/market_supervision_agent/config/local_ai_config.yaml"
            config_path = default_path

        try:
            with open(config_path, 'r', encoding='utf-8') as f:
                return yaml.safe_load(f)
        except FileNotFoundError:
            # è¿”å›é»˜è®¤é…ç½®
            return self._get_default_config()

    def _get_default_config(self) -> Dict:
        """è·å–é»˜è®¤é…ç½®"""
        return {
            'ocr': {
                'primary_engine': 'paddleocr',
                'fallback_engine': 'baidu',
                'paddleocr': {
                    'use_gpu': False,
                    'lang': 'ch',
                    'use_angle_cls': True,
                    'show_log': False,
                    'enable_mkldnn': True,
                    'mem_optim': True
                }
            },
            'embedding': {
                'model_name': 'sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2',
                'device': 'cpu',
                'batch_size': 32
            },
            'llm': {
                'primary': 'deepseek-chat',
                'api': {
                    'provider': 'deepseek',
                    'base_url': 'https://api.deepseek.com/v1',
                    'model': 'deepseek-chat'
                }
            },
            'performance': {
                'cache': {'enabled': True, 'max_size': 1000}
            },
            'logging': {
                'level': 'INFO',
                'console': {'enabled': True}
            }
        }

    def _setup_logging(self) -> logging.Logger:
        """è®¾ç½®æ—¥å¿—"""
        logger = logging.getLogger('LocalAIEngine')
        logger.setLevel(self.config.get('logging', {}).get('level', 'INFO'))

        # æ§åˆ¶å°å¤„ç†å™¨
        if self.config.get('logging', {}).get('console', {}).get('enabled', True):
            handler = logging.StreamHandler()
            formatter = logging.Formatter(
                '%(asctime)s - %(name)s - %(levelname)s - %(message)s'
            )
            handler.setFormatter(formatter)
            logger.addHandler(handler)

        return logger

    def _initialize_engines(self):
        """åˆå§‹åŒ– AI å¼•æ“"""
        # OCR å¼•æ“
        if self.config['ocr']['primary_engine'] == 'paddleocr':
            self._init_paddleocr()

        # åµŒå…¥æ¨¡å‹
        self._init_embedding_model()

        self.logger.info("âœ… æœ¬åœ° AI å¼•æ“åˆå§‹åŒ–å®Œæˆ")

    def _init_paddleocr(self):
        """åˆå§‹åŒ– PaddleOCR"""
        try:
            from paddleocr import PaddleOCR

            config = self.config['ocr']['paddleocr']
            use_gpu = config.get('use_gpu', False)

            # åˆ›å»º PaddleOCR å®ä¾‹
            ocr = PaddleOCR(
                use_angle_cls=config.get('use_angle_cls', True),
                lang=config.get('lang', 'ch'),
                use_gpu=use_gpu,
                show_log=config.get('show_log', False),
                enable_mkldnn=config.get('enable_mkldnn', True)
            )

            self._engines[AIEngine.PADDLEOCR] = ocr

            gpu_status = "GPU" if use_gpu else "CPU"
            self.logger.info(f"âœ… PaddleOCR åˆå§‹åŒ–æˆåŠŸ ({gpu_status})")

        except ImportError:
            self.logger.warning("âš ï¸ PaddleOCR æœªå®‰è£…ï¼Œè¿è¡Œ: pip install paddleocr")
        except Exception as e:
            self.logger.error(f"âŒ PaddleOCR åˆå§‹åŒ–å¤±è´¥: {e}")

    def _init_embedding_model(self):
        """åˆå§‹åŒ–åµŒå…¥æ¨¡å‹"""
        try:
            from sentence_transformers import SentenceTransformer

            config = self.config['embedding']
            model_name = config.get('model_name',
                                   'sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2')
            device = config.get('device', 'cpu')

            model = SentenceTransformer(model_name, device=device)
            self._engines[AIEngine.SENTENCE_TRANSFORMER] = model

            self.logger.info(f"âœ… åµŒå…¥æ¨¡å‹åˆå§‹åŒ–æˆåŠŸ ({device})")

        except ImportError:
            self.logger.warning("âš ï¸ sentence-transformers æœªå®‰è£…")
        except Exception as e:
            self.logger.error(f"âŒ åµŒå…¥æ¨¡å‹åˆå§‹åŒ–å¤±è´¥: {e}")

    def ocr_extract(
        self,
        image_path: str,
        use_fallback: bool = True
    ) -> OCRResult:
        """
        OCR æ–‡æœ¬æå–

        Args:
            image_path: å›¾ç‰‡è·¯å¾„
            use_fallback: æ˜¯å¦ä½¿ç”¨å¤‡ç”¨å¼•æ“

        Returns:
            OCR ç»“æœ
        """
        start_time = time.time()

        # æ£€æŸ¥ç¼“å­˜
        cache_key = f"ocr_{image_path}"
        if self._is_cache_enabled() and cache_key in self._cache:
            self.logger.info(f"ğŸ“¦ ä»ç¼“å­˜è¯»å–: {image_path}")
            return self._cache[cache_key]

        # å°è¯•ä¸»å¼•æ“
        result = self._ocr_with_primary(image_path)

        # å¦‚æœä¸»å¼•æ“å¤±è´¥ä¸”å…è®¸é™çº§
        if result.confidence < 0.7 and use_fallback:
            self.logger.warning("âš ï¸ ä¸»å¼•æ“ç½®ä¿¡åº¦ä½ï¼Œå°è¯•å¤‡ç”¨å¼•æ“")
            result = self._ocr_with_fallback(image_path)

        # ç¼“å­˜ç»“æœ
        if self._is_cache_enabled():
            self._cache[cache_key] = result

        processing_time = time.time() - start_time
        result.processing_time = processing_time

        self.logger.info(f"âœ… OCR å®Œæˆ (è€—æ—¶: {processing_time:.2f}s, ç½®ä¿¡åº¦: {result.confidence:.2f})")

        return result

    def _ocr_with_primary(self, image_path: str) -> OCRResult:
        """ä½¿ç”¨ä¸»å¼•æ“è¿›è¡Œ OCR"""
        primary = self.config['ocr']['primary_engine']

        if primary == 'paddleocr':
            return self._ocr_paddleocr(image_path)
        else:
            raise ValueError(f"æœªçŸ¥çš„ä¸»å¼•æ“: {primary}")

    def _ocr_paddleocr(self, image_path: str) -> OCRResult:
        """PaddleOCR è¯†åˆ«"""
        if AIEngine.PADDLEOCR not in self._engines:
            return OCRResult(
                text="",
                confidence=0.0,
                engine="none",
                processing_time=0.0
            )

        ocr = self._engines[AIEngine.PADDLEOCR]
        result = ocr.ocr(image_path, cls=True)

        if not result or not result[0]:
            return OCRResult(
                text="",
                confidence=0.0,
                engine="paddleocr",
                processing_time=0.0
            )

        # æå–æ–‡æœ¬å’Œç½®ä¿¡åº¦
        texts = []
        confidences = []

        for line in result[0]:
            if line:
                bbox, (text, confidence) = line
                texts.append(text)
                confidences.append(confidence)

        full_text = '\n'.join(texts)
        avg_confidence = sum(confidences) / len(confidences) if confidences else 0.0

        return OCRResult(
            text=full_text,
            confidence=avg_confidence,
            engine="paddleocr",
            processing_time=0.0,
            raw_data={'result': result}
        )

    def _ocr_with_fallback(self, image_path: str) -> OCRResult:
        """ä½¿ç”¨å¤‡ç”¨å¼•æ“"""
        fallback = self.config['ocr']['fallback_engine']

        if fallback == 'baidu':
            # è¿™é‡Œå¯ä»¥é›†æˆç™¾åº¦ OCR
            return OCRResult(
                text="",
                confidence=0.0,
                engine="baidu",
                processing_time=0.0
            )

        return OCRResult(
            text="",
            confidence=0.0,
            engine="none",
            processing_time=0.0
        )

    def embed_text(self, texts: List[str]) -> List[List[float]]:
        """
        æ–‡æœ¬åµŒå…¥

        Args:
            texts: æ–‡æœ¬åˆ—è¡¨

        Returns:
            åµŒå…¥å‘é‡åˆ—è¡¨
        """
        if AIEngine.SENTENCE_TRANSFORMER not in self._engines:
            self.logger.error("âŒ åµŒå…¥æ¨¡å‹æœªåˆå§‹åŒ–")
            return []

        model = self._engines[AIEngine.SENTENCE_TRANSFORMER]
        embeddings = model.encode(
            texts,
            batch_size=self.config['embedding'].get('batch_size', 32),
            show_progress_bar=False
        )

        return embeddings.tolist()

    def semantic_search(
        self,
        query: str,
        documents: List[str],
        top_k: int = 5
    ) -> List[Tuple[str, float]]:
        """
        è¯­ä¹‰æœç´¢

        Args:
            query: æŸ¥è¯¢æ–‡æœ¬
            documents: æ–‡æ¡£åˆ—è¡¨
            top_k: è¿”å›å‰ K ä¸ªç»“æœ

        Returns:
            [(æ–‡æ¡£, ç›¸ä¼¼åº¦), ...]
        """
        # ç”ŸæˆåµŒå…¥
        query_emb = self.embed_text([query])[0]
        doc_embs = self.embed_text(documents)

        # è®¡ç®—ç›¸ä¼¼åº¦
        from sklearn.metrics.pairwise import cosine_similarity
        import numpy as np

        similarities = cosine_similarity(
            [query_emb],
            doc_embs
        )[0]

        # æ’åº
        indices = np.argsort(similarities)[::-1][:top_k]

        results = [
            (documents[i], similarities[i])
            for i in indices
        ]

        return results

    def _is_cache_enabled(self) -> bool:
        """æ£€æŸ¥æ˜¯å¦å¯ç”¨ç¼“å­˜"""
        return self.config.get('performance', {}).get('cache', {}).get('enabled', False)

    def get_stats(self) -> Dict[str, Any]:
        """è·å–å¼•æ“ç»Ÿè®¡ä¿¡æ¯"""
        return {
            'engines_loaded': list(self._engines.keys()),
            'cache_size': len(self._cache),
            'config': self.config
        }


# ä½¿ç”¨ç¤ºä¾‹
if __name__ == "__main__":
    # åˆ›å»ºå¼•æ“
    engine = LocalAIEngine()

    # æµ‹è¯• OCR
    print("\n=== æœ¬åœ° AI å¼•æ“æµ‹è¯• ===\n")

    # æŸ¥çœ‹ç»Ÿè®¡ä¿¡æ¯
    stats = engine.get_stats()
    print(f"ğŸ“Š å·²åŠ è½½å¼•æ“: {stats['engines_loaded']}")
    print(f"ğŸ“¦ ç¼“å­˜å¤§å°: {stats['cache_size']}")

    print("\nâœ… æœ¬åœ° AI å¼•æ“å·²å‡†å¤‡å°±ç»ªï¼")
    print("\nğŸ“‹ å¯ç”¨åŠŸèƒ½:")
    print("  1. OCR æ–‡æœ¬è¯†åˆ«ï¼ˆPaddleOCRï¼‰")
    print("  2. æ–‡æœ¬åµŒå…¥ï¼ˆSentence Transformersï¼‰")
    print("  3. è¯­ä¹‰æœç´¢")
    print("  4. æ™ºèƒ½ç¼“å­˜")
