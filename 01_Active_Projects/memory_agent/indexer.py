#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
ç¬”è®°ç´¢å¼•å™¨
æ‰«æå­¦ä¹ ç¬”è®°å’Œé¡¹ç›®ä»£ç ï¼Œç”Ÿæˆå‘é‡åµŒå…¥å¹¶å­˜å…¥æ•°æ®åº“
"""

import yaml
import hashlib
from pathlib import Path
from datetime import datetime
from typing import List, Dict, Tuple
from tqdm import tqdm

from embedder import TextEmbedder
from vector_store import VectorStore


class DocumentIndexer:
    """æ–‡æ¡£ç´¢å¼•å™¨ - æ‰«æå’Œç´¢å¼•å­¦ä¹ èµ„æ–™"""

    def __init__(self, config_path="config.yaml"):
        """åˆå§‹åŒ–ç´¢å¼•å™¨"""
        # åŠ è½½é…ç½®
        config_path = Path(__file__).parent / config_path
        with open(config_path, 'r', encoding='utf-8') as f:
            self.config = yaml.safe_load(f)

        self.workspace_root = Path(__file__).parent.parent.parent

        # åˆå§‹åŒ–åµŒå…¥å™¨å’Œå‘é‡æ•°æ®åº“
        self.embedder = TextEmbedder(config_path)
        self.vector_store = VectorStore(config_path)

        # ç»Ÿè®¡ä¿¡æ¯
        self.stats = {
            'indexed': 0,
            'skipped': 0,
            'failed': 0
        }

    def scan_sources(self) -> List[Dict]:
        """
        æ‰«ææ‰€æœ‰é…ç½®çš„æºç›®å½•

        Returns:
            æ–‡æ¡£ä¿¡æ¯åˆ—è¡¨
        """
        all_docs = []

        # æ‰«æå­¦ä¹ æ—¥å¿—
        all_docs.extend(self._scan_learning_journal())

        # æ‰«æé¡¹ç›®ä»£ç 
        all_docs.extend(self._scan_projects())

        return all_docs

    def _scan_learning_journal(self) -> List[Dict]:
        """æ‰«æå­¦ä¹ æ—¥å¿—ç›®å½•"""
        source_config = self.config['sources']['learning_journal']
        base_path = self.workspace_root / source_config['path']

        if not base_path.exists():
            print(f"âš ï¸  å­¦ä¹ æ—¥å¿—ç›®å½•ä¸å­˜åœ¨: {base_path}")
            return []

        print(f"\nğŸ“š æ‰«æå­¦ä¹ æ—¥å¿—: {base_path}")

        docs = []
        patterns = source_config['patterns']
        exclude_patterns = source_config.get('exclude_patterns', [])

        for pattern in patterns:
            matched_files = list(base_path.glob(pattern))

            for file_path in tqdm(matched_files, desc=f"  æ‰«æ {pattern}"):
                # æ£€æŸ¥æ’é™¤è§„åˆ™
                if self._should_exclude(file_path, exclude_patterns):
                    continue

                # è¯»å–æ–‡ä»¶
                content = self._read_file(file_path)
                if content is None:
                    continue

                # è§£æå…ƒæ•°æ®
                metadata = self._parse_metadata(file_path, content, 'journal')

                docs.append({
                    'content': content,
                    'metadata': metadata,
                    'file_path': file_path
                })

        print(f"   æ‰¾åˆ° {len(docs)} ä¸ªå­¦ä¹ ç¬”è®°æ–‡ä»¶")
        return docs

    def _scan_projects(self) -> List[Dict]:
        """æ‰«æé¡¹ç›®ä»£ç """
        source_config = self.config['sources']['projects']
        base_path = self.workspace_root / source_config['path']

        if not base_path.exists():
            print(f"âš ï¸  é¡¹ç›®ç›®å½•ä¸å­˜åœ¨: {base_path}")
            return []

        print(f"\nğŸ’» æ‰«æé¡¹ç›®ä»£ç : {base_path}")

        docs = []
        patterns = source_config['patterns']
        exclude_patterns = source_config.get('exclude_patterns', [])

        for pattern in patterns:
            matched_files = list(base_path.glob(pattern))

            for file_path in tqdm(matched_files, desc=f"  æ‰«æ {pattern}"):
                # æ£€æŸ¥æ’é™¤è§„åˆ™
                if self._should_exclude(file_path, exclude_patterns):
                    continue

                # è¯»å–æ–‡ä»¶
                content = self._read_file(file_path)
                if content is None:
                    continue

                # è§£æå…ƒæ•°æ®
                metadata = self._parse_metadata(file_path, content, 'project')

                docs.append({
                    'content': content,
                    'metadata': metadata,
                    'file_path': file_path
                })

        print(f"   æ‰¾åˆ° {len(docs)} ä¸ªé¡¹ç›®æ–‡ä»¶")
        return docs

    def _should_exclude(self, file_path: Path, exclude_patterns: List[str]) -> bool:
        """æ£€æŸ¥æ–‡ä»¶æ˜¯å¦åº”è¯¥è¢«æ’é™¤"""
        file_str = str(file_path)

        for pattern in exclude_patterns:
            if pattern in file_str:
                return True

        return False

    def _read_file(self, file_path: Path) -> str:
        """è¯»å–æ–‡ä»¶å†…å®¹"""
        try:
            with open(file_path, 'r', encoding='utf-8') as f:
                return f.read()
        except Exception as e:
            print(f"âŒ è¯»å–æ–‡ä»¶å¤±è´¥ {file_path}: {e}")
            self.stats['failed'] += 1
            return None

    def _parse_metadata(self, file_path: Path, content: str, doc_type: str) -> Dict:
        """è§£ææ–‡æ¡£å…ƒæ•°æ®"""
        # ç›¸å¯¹è·¯å¾„ï¼ˆä¾¿äºæ˜¾ç¤ºï¼‰
        rel_path = file_path.relative_to(self.workspace_root)

        # æ–‡ä»¶ä¿®æ”¹æ—¶é—´
        mtime = datetime.fromtimestamp(file_path.stat().st_mtime)

        # æå–æ ‡é¢˜ï¼ˆMarkdownæ–‡ä»¶çš„ç¬¬ä¸€è¡Œ#æ ‡é¢˜ï¼‰
        title = file_path.stem
        if doc_type == 'journal' and content.strip().startswith('#'):
            first_line = content.strip().split('\n')[0]
            if first_line.startswith('#'):
                title = first_line.lstrip('#').strip()

        metadata = {
            'title': title,
            'path': str(rel_path),
            'type': doc_type,
            'extension': file_path.suffix,
            'modified': mtime.strftime("%Y-%m-%d %H:%M:%S"),
            'modified_timestamp': mtime.timestamp()
        }

        # æå–åˆ†ç±»ï¼ˆåŸºäºç›®å½•ç»“æ„ï¼‰
        path_parts = rel_path.parts
        if len(path_parts) > 2:
            metadata['category'] = path_parts[2]  # ä¾‹å¦‚: daily_logs, challenges_solved

        return metadata

    def index_documents(self, docs: List[Dict], batch_size: int = 32):
        """
        æ‰¹é‡ç´¢å¼•æ–‡æ¡£

        Args:
            docs: æ–‡æ¡£åˆ—è¡¨
            batch_size: æ‰¹å¤„ç†å¤§å°
        """
        if not docs:
            print("âš ï¸  æ²¡æœ‰æ–‡æ¡£éœ€è¦ç´¢å¼•")
            return

        print(f"\nğŸ”„ å¼€å§‹ç´¢å¼• {len(docs)} ä¸ªæ–‡æ¡£...")

        # åˆ†æ‰¹å¤„ç†
        for i in tqdm(range(0, len(docs), batch_size), desc="ç”ŸæˆåµŒå…¥å‘é‡"):
            batch = docs[i:i + batch_size]

            # æå–å†…å®¹
            contents = [doc['content'] for doc in batch]
            metadatas = [doc['metadata'] for doc in batch]

            # ç”ŸæˆIDï¼ˆåŸºäºæ–‡ä»¶è·¯å¾„çš„å“ˆå¸Œï¼‰
            ids = [hashlib.md5(str(doc['file_path']).encode()).hexdigest() for doc in batch]

            # ç”ŸæˆåµŒå…¥
            embeddings = self.embedder.embed_texts(contents)

            # æ£€æŸ¥æ˜¯å¦å·²å­˜åœ¨
            existing_docs = []
            new_contents = []
            new_embeddings = []
            new_metadatas = []
            new_ids = []

            for j, (content, emb, meta, doc_id) in enumerate(zip(contents, embeddings, metadatas, ids)):
                existing = self.vector_store.get_document(doc_id)

                # å¦‚æœå·²å­˜åœ¨ï¼Œæ›´æ–°
                if existing:
                    self.vector_store.update_document(
                        doc_id=doc_id,
                        document=content,
                        embedding=emb,
                        metadata=meta
                    )
                    self.stats['indexed'] += 1
                else:
                    # æ–°æ–‡æ¡£
                    new_contents.append(content)
                    new_embeddings.append(emb)
                    new_metadatas.append(meta)
                    new_ids.append(doc_id)

            # æ·»åŠ æ–°æ–‡æ¡£
            if new_contents:
                self.vector_store.add_documents(
                    documents=new_contents,
                    embeddings=new_embeddings,
                    metadatas=new_metadatas,
                    ids=new_ids
                )
                self.stats['indexed'] += len(new_contents)

    def build_index(self):
        """æ„å»ºå®Œæ•´ç´¢å¼•"""
        print("\n" + "=" * 70)
        print("ğŸš€ å¼€å§‹æ„å»ºå­¦ä¹ è®°å¿†ç´¢å¼•")
        print("=" * 70)

        # æ‰«ææ–‡æ¡£
        docs = self.scan_sources()

        if not docs:
            print("\nâš ï¸  æœªæ‰¾åˆ°ä»»ä½•æ–‡æ¡£")
            return

        # ç´¢å¼•æ–‡æ¡£
        self.index_documents(docs)

        # æ˜¾ç¤ºç»Ÿè®¡
        print("\n" + "=" * 70)
        print("ğŸ“Š ç´¢å¼•å®Œæˆ")
        print("=" * 70)
        print(f"âœ… æˆåŠŸç´¢å¼•: {self.stats['indexed']} ä¸ª")
        print(f"â­ï¸  è·³è¿‡: {self.stats['skipped']} ä¸ª")
        print(f"âŒ å¤±è´¥: {self.stats['failed']} ä¸ª")
        print(f"\nğŸ“š æ€»æ–‡æ¡£æ•°: {self.vector_store.count()}")


def main():
    """ä¸»å‡½æ•°"""
    indexer = DocumentIndexer()
    indexer.build_index()


if __name__ == "__main__":
    main()
